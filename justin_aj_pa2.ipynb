{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1n3hGSNX7NX"
      },
      "source": [
        "# Programming assignment 2: Convolutional Neural Networks (100 points)\n",
        "\n",
        "## Overview\n",
        "<font size='4'> In this assignment you will practice putting together a Convolution Neural Network (CNN) classification pipeline. So far we have worked with deep fully-connected networks, using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are computationally efficient, but in practice CNNs work better for image classification.\n",
        "\n",
        "<font size='4'>In the first part , you will implement several layer types that are used in convolutional networks using Numpy. In the second part, you will then implement a custom CNN and ones based on the ResNet using PyTorch. You will also practice hyper parameter tuning to achieve desired accuracy.\n",
        "\n",
        "## Submission format\n",
        "* <font size='4'>`<your_nu_username>_pa2.ipynb`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-bQYoL5X7NY"
      },
      "source": [
        "## setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DnjiPjyrX7NZ"
      },
      "outputs": [],
      "source": [
        "# As usual, a bit of setup\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "def rel_error(x, y):\n",
        "    \"\"\" returns relative error \"\"\"\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mdfI_ypX7Na",
        "outputId": "557138e3-8732-4597-8588-de4f3865de33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  162M  100  162M    0     0  28.3M      0  0:00:05  0:00:05 --:--:-- 31.1M\n",
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n",
            "/bin/bash: line 1: del: command not found\n"
          ]
        }
      ],
      "source": [
        "# let's download the data\n",
        "# !mkdir ../datasets\n",
        "# !cd ../datasets\n",
        "\n",
        "# 1 -- Linux\n",
        "# 2 -- MacOS\n",
        "# 3 -- Command Prompt on Windows\n",
        "# 4 -- manually downloading the data\n",
        "choice = 3\n",
        "\n",
        "\n",
        "if choice == 1:\n",
        "    # should work well on Linux and in Powershell on Windows\n",
        "    !wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "elif choice == 2 or choice ==3:\n",
        "    # if wget is not available for you, try curl\n",
        "    # should work well on MacOS\n",
        "    !curl http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz --output cifar-10-python.tar.gz\n",
        "else:\n",
        "    print('Please manually download the data from http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz and put it under the datasets folder.')\n",
        "!tar -xzvf cifar-10-python.tar.gz\n",
        "\n",
        "if choice==3:\n",
        "    !del cifar-10-python.tar.gz\n",
        "else:\n",
        "    !rm cifar-10-python.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XnslsToKX7Na"
      },
      "outputs": [],
      "source": [
        "# helpful functions to process and load the data\n",
        "from six.moves import cPickle as pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from imageio import imread\n",
        "import platform\n",
        "\n",
        "def load_pickle(f):\n",
        "    version = platform.python_version_tuple()\n",
        "    if version[0] == '2':\n",
        "        return  pickle.load(f)\n",
        "    elif version[0] == '3':\n",
        "        return  pickle.load(f, encoding='latin1')\n",
        "    raise ValueError(\"invalid python version: {}\".format(version))\n",
        "\n",
        "def load_CIFAR_batch(filename):\n",
        "  \"\"\" load single batch of cifar \"\"\"\n",
        "  with open(filename, 'rb') as f:\n",
        "    datadict = load_pickle(f)\n",
        "    X = datadict['data']\n",
        "    Y = datadict['labels']\n",
        "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
        "    Y = np.array(Y)\n",
        "    return X, Y\n",
        "\n",
        "def load_CIFAR10(ROOT):\n",
        "  \"\"\" load all of cifar \"\"\"\n",
        "  xs = []\n",
        "  ys = []\n",
        "  for b in range(1,6):\n",
        "    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
        "    X, Y = load_CIFAR_batch(f)\n",
        "    xs.append(X)\n",
        "    ys.append(Y)\n",
        "  Xtr = np.concatenate(xs)\n",
        "  Ytr = np.concatenate(ys)\n",
        "  del X, Y\n",
        "  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
        "  return Xtr, Ytr, Xte, Yte\n",
        "\n",
        "\n",
        "def get_CIFAR10_data(cifar10_dir, num_training=49000, num_validation=1000, num_test=1000,\n",
        "                     subtract_mean=True):\n",
        "    \"\"\"\n",
        "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "    it for classifiers. These are the same steps as we used for the SVM, but\n",
        "    condensed to a single function.\n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "\n",
        "    # Subsample the data\n",
        "    mask = list(range(num_training, num_training + num_validation))\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = list(range(num_training))\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = list(range(num_test))\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "\n",
        "    # Normalize the data: subtract the mean image\n",
        "    if subtract_mean:\n",
        "      mean_image = np.mean(X_train, axis=0)\n",
        "      X_train -= mean_image\n",
        "      X_val -= mean_image\n",
        "      X_test -= mean_image\n",
        "\n",
        "    # Transpose so that channels come first\n",
        "    X_train = X_train.transpose(0, 3, 1, 2).copy()\n",
        "    X_val = X_val.transpose(0, 3, 1, 2).copy()\n",
        "    X_test = X_test.transpose(0, 3, 1, 2).copy()\n",
        "\n",
        "    # Package data into a dictionary\n",
        "    return {\n",
        "      'X_train': X_train, 'y_train': y_train,\n",
        "      'X_val': X_val, 'y_val': y_val,\n",
        "      'X_test': X_test, 'y_test': y_test,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw9bYKMefeRK",
        "outputId": "b8a8138f-58f3-4904-b4ff-aacafb0481c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===\n",
            "For the split train\n",
            "shape: (49000, 3, 32, 32)\n",
            "data value range, min: -4.489820571085577, max: 0.8966644435551998\n",
            "\n",
            "===\n",
            "For the split val\n",
            "shape: (1000, 3, 32, 32)\n",
            "data value range, min: -4.489820571085577, max: 0.8966644435551998\n",
            "\n",
            "===\n",
            "For the split test\n",
            "shape: (1000, 3, 32, 32)\n",
            "data value range, min: -4.489820571085577, max: 0.8966644435551998\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the (preprocessed) CIFAR10 data.\n",
        "cifar10_dir = './cifar-10-batches-py'\n",
        "\n",
        "data = get_CIFAR10_data(cifar10_dir, subtract_mean=True)\n",
        "\n",
        "pix_mean = (0.485, 0.456, 0.406)\n",
        "pix_std = (0.229, 0.224, 0.225)\n",
        "\n",
        "for c in range(3):\n",
        "    data['X_train'][:, c] = (data['X_train'][:, c] / 255 - pix_mean[c]) / pix_std[c]\n",
        "    data['X_val'][:, c] = (data['X_val'][:, c] / 255 - pix_mean[c]) / pix_std[c]\n",
        "    data['X_test'][:, c] = (data['X_test'][:, c] / 255 - pix_mean[c]) / pix_std[c]\n",
        "\n",
        "for split in ['train', 'val', 'test']:\n",
        "    print('===\\nFor the split {}'.format(split))\n",
        "    print('shape: {}'.format(data['X_{}'.format(split)].shape))\n",
        "    print('data value range, min: {}, max: {}\\n'.format(data['X_{}'.format(split)].min(), data['X_{}'.format(split)].max()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMLkz8Y2X7Na"
      },
      "source": [
        "## Part 1: Implementing convolution and batch normalization layers using Numpy (25 points)\n",
        "(adapted from the work done by Erik Learned-Miller, which was originally developed by Fei-Fei Li, Andrej Karpathy, and Justin Johnson)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2Cz4RqdX7Na"
      },
      "source": [
        "<font size=\"4\" color=\"red\">**task 1.1: forward pass of a convolution layer with two nested for loops (10 points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dq9kNQomX7Na"
      },
      "outputs": [],
      "source": [
        "def conv_forward_naive(x, w, b, conv_param):\n",
        "    \"\"\"\n",
        "    A naive implementation of the forward pass for a convolutional layer.\n",
        "\n",
        "    The input consists of N data points, each with C channels, height H and\n",
        "    width W. We convolve each input with F different filters, where each filter\n",
        "    spans all C channels and has height HH and width WW.\n",
        "\n",
        "    Input:\n",
        "    - x: Input data of shape (N, C, H, W)\n",
        "    - w: Filter weights of shape (F, C, HH, WW)\n",
        "    - b: Biases, of shape (F,)\n",
        "    - conv_param: A dictionary with the following keys:\n",
        "      - 'stride': The number of pixels between adjacent receptive fields in the\n",
        "        horizontal and vertical directions.\n",
        "      - 'pad': The number of pixels that will be used to zero-pad the input.\n",
        "\n",
        "\n",
        "    During padding, 'pad' zeros should be placed symmetrically (i.e equally on both sides)\n",
        "    along the height and width axes of the input. Be careful not to modfiy the original\n",
        "    input x directly.\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
        "      H' = ceil((H + 2 * pad - HH + 1) / stride)\n",
        "      W' = ceil((W + 2 * pad - WW + 1) / stride)\n",
        "    - cache: (x, w, b, conv_param)\n",
        "    \"\"\"\n",
        "    out = None\n",
        "\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the convolutional forward pass.                         #\n",
        "    # Hint: you can use the function np.pad for padding.                      #\n",
        "    ###########################################################################\n",
        "\n",
        "    stride = conv_param['stride']\n",
        "    pad = conv_param['pad']\n",
        "\n",
        "    N, C, H, W = x.shape\n",
        "    F, C, HH, WW = w.shape\n",
        "\n",
        "    H_out = int(np.ceil((H + 2 * pad - HH + 1) / stride))\n",
        "    W_out = int(np.ceil((W + 2 * pad - WW + 1) / stride))\n",
        "\n",
        "    x_pad = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant', constant_values=0)\n",
        "\n",
        "    out = np.zeros((N, F, H_out, W_out))\n",
        "\n",
        "    for h in range(H_out):\n",
        "        for w_ in range(W_out):\n",
        "            h_start, w_start = h * stride, w_ * stride\n",
        "            x_slice = x_pad[:, :, h_start:h_start+HH, w_start:w_start+WW]\n",
        "            out[:, :, h, w_] = np.tensordot(x_slice, w, axes=([1, 2, 3], [1, 2, 3])) + b\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "\n",
        "    cache = (x, w, b, conv_param)\n",
        "    return out, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-xIe8azX7Na",
        "outputId": "fe5733f2-dcb1-4953-b9fd-4222886bc612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing conv_forward_naive\n",
            "difference:  2.2121476417505994e-08\n"
          ]
        }
      ],
      "source": [
        "# check your forward pass implementation\n",
        "x_shape = (2, 3, 4, 4)\n",
        "w_shape = (3, 3, 4, 4)\n",
        "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
        "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
        "b = np.linspace(-0.1, 0.2, num=3)\n",
        "\n",
        "conv_param = {'stride': 2, 'pad': 1}\n",
        "out, _ = conv_forward_naive(x, w, b, conv_param)\n",
        "correct_out = np.array([[[[-0.08759809, -0.10987781],\n",
        "                           [-0.18387192, -0.2109216 ]],\n",
        "                          [[ 0.21027089,  0.21661097],\n",
        "                           [ 0.22847626,  0.23004637]],\n",
        "                          [[ 0.50813986,  0.54309974],\n",
        "                           [ 0.64082444,  0.67101435]]],\n",
        "                         [[[-0.98053589, -1.03143541],\n",
        "                           [-1.19128892, -1.24695841]],\n",
        "                          [[ 0.69108355,  0.66880383],\n",
        "                           [ 0.59480972,  0.56776003]],\n",
        "                          [[ 2.36270298,  2.36904306],\n",
        "                           [ 2.38090835,  2.38247847]]]])\n",
        "\n",
        "# Compare your output to ours; difference should be around e-8\n",
        "print('Testing conv_forward_naive')\n",
        "print('difference: ', rel_error(out, correct_out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOOLqcs1X7Nb"
      },
      "source": [
        "<font size='4' color='red'>**Task 1.2: forward pass of a (normal) batch norm layer (10 points).**\n",
        "\n",
        "<font size='4'>Batch normalization is a very useful technique for training deep neural networks. As proposed in the original paper [1], batch normalization can also be used for convolutional networks, but we need to tweak it a bit; the modification will be called \"spatial batch normalization.\"\n",
        "\n",
        "<font size='4'>Normally batch-normalization accepts inputs of shape `(N, D)` and produces outputs of shape `(N, D)`, where we normalize across the minibatch dimension `N`.\n",
        "\n",
        "[1] [Sergey Ioffe and Christian Szegedy, \"Batch Normalization: Accelerating Deep Network Training by Reducing\n",
        "Internal Covariate Shift\", ICML 2015.](https://arxiv.org/abs/1502.03167)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XT_sIvjPX7Nb"
      },
      "outputs": [],
      "source": [
        "def batchnorm_forward(x, gamma, beta, bn_param):\n",
        "    \"\"\"\n",
        "    Forward pass for batch normalization.\n",
        "\n",
        "    During training the sample mean and (uncorrected) sample variance are\n",
        "    computed from minibatch statistics and used to normalize the incoming data.\n",
        "    During training we also keep an exponentially decaying running mean of the\n",
        "    mean and variance of each feature, and these averages are used to normalize\n",
        "    data at test-time.\n",
        "\n",
        "    At each timestep we update the running averages for mean and variance using\n",
        "    an exponential decay based on the momentum parameter:\n",
        "\n",
        "    running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
        "    running_var = momentum * running_var + (1 - momentum) * sample_var\n",
        "\n",
        "    Note that the batch normalization paper suggests a different test-time\n",
        "    behavior: they compute sample mean and variance for each feature using a\n",
        "    large number of training images rather than using a running average. For\n",
        "    this implementation we have chosen to use running averages instead since\n",
        "    they do not require an additional estimation step; the PyTorch\n",
        "    implementation of batch normalization also uses running averages.\n",
        "\n",
        "    Input:\n",
        "    - x: Data of shape (N, D)\n",
        "    - gamma: Scale parameter of shape (D,)\n",
        "    - beta: Shift paremeter of shape (D,)\n",
        "    - bn_param: Dictionary with the following keys:\n",
        "      - mode: 'train' or 'test'; required\n",
        "      - eps: Constant for numeric stability\n",
        "      - momentum: Constant for running mean / variance.\n",
        "      - running_mean: Array of shape (D,) giving running mean of features\n",
        "      - running_var Array of shape (D,) giving running variance of features\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: of shape (N, D)\n",
        "    - cache: A tuple of values needed in the backward pass\n",
        "    \"\"\"\n",
        "    mode = bn_param['mode']\n",
        "    eps = bn_param.get('eps', 1e-5)\n",
        "    momentum = bn_param.get('momentum', 0.9)\n",
        "\n",
        "    N, D = x.shape\n",
        "    running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype))\n",
        "    running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype))\n",
        "\n",
        "    out, cache = None, None\n",
        "    if mode == 'train':\n",
        "        #######################################################################\n",
        "        # TODO: Implement the training-time forward pass for batch norm.      #\n",
        "        # Use minibatch statistics to compute the mean and variance, use      #\n",
        "        # these statistics to normalize the incoming data, and scale and      #\n",
        "        # shift the normalized data using gamma and beta. Simply treat the    #\n",
        "        # sample mean and sample variance as constants to simplify the        #\n",
        "        # gradients computation.                                              #\n",
        "        #                                                                     #\n",
        "        # You should store the output in the variable out. Any intermediates  #\n",
        "        # that you need for the backward pass should be stored in the cache   #\n",
        "        # variable.                                                           #\n",
        "        #                                                                     #\n",
        "        # You should also use your computed sample mean and variance together #\n",
        "        # with the momentum variable to update the running mean and running   #\n",
        "        # variance, storing your result in the running_mean and running_var   #\n",
        "        # variables.                                                          #\n",
        "        #                                                                     #\n",
        "        # Note that though you should be keeping track of the running         #\n",
        "        # variance, you should normalize the data based on the standard       #\n",
        "        # deviation (square root of variance) instead!                        #\n",
        "        # Referencing the original paper (https://arxiv.org/abs/1502.03167)   #\n",
        "        # might prove to be helpful.                                          #\n",
        "        #######################################################################\n",
        "        # raise NotImplementedError\n",
        "\n",
        "        sample_mean = np.mean(x, axis=0)\n",
        "        sample_var = np.var(x, axis=0)\n",
        "\n",
        "        x_ = (x - sample_mean) / np.sqrt(sample_var + eps)\n",
        "\n",
        "        out = gamma * x_ + beta\n",
        "\n",
        "        running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
        "        running_var = momentum * running_var + (1 - momentum) * sample_var\n",
        "\n",
        "        cache = (x, sample_mean, sample_var, x_, gamma, beta, eps)\n",
        "\n",
        "        bn_param['running_mean'] = running_mean\n",
        "        bn_param['running_var'] = running_var\n",
        "\n",
        "        #######################################################################\n",
        "        #                           END OF YOUR CODE                          #\n",
        "        #######################################################################\n",
        "    elif mode == 'test':\n",
        "        #######################################################################\n",
        "        # TODO: Implement the test-time forward pass for batch normalization. #\n",
        "        # Use the running mean and variance to normalize the incoming data,   #\n",
        "        # then scale and shift the normalized data using gamma and beta.      #\n",
        "        # Store the result in the out variable.                               #\n",
        "        #######################################################################\n",
        "        x_ = (x - running_mean) / np.sqrt(running_var + eps)\n",
        "        out = gamma * x_ + beta\n",
        "        # raise NotImplementedError\n",
        "        #######################################################################\n",
        "        #                          END OF YOUR CODE                           #\n",
        "        #######################################################################\n",
        "    else:\n",
        "        raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n",
        "\n",
        "    # Store the updated running means back into bn_param\n",
        "    bn_param['running_mean'] = running_mean\n",
        "    bn_param['running_var'] = running_var\n",
        "\n",
        "    return out, cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-YNqB7LX7Nb"
      },
      "source": [
        "<font size='4' color='red'>**Task 1.3: forward pass of a spatial batch norm layer (5 points).**\n",
        "\n",
        "<font size='4'>For data coming from convolutional layers, batch normalization needs to accept inputs of shape `(N, C, H, W)` and produce outputs of shape `(N, C, H, W)` where the `N` dimension gives the minibatch size and the `(H, W)` dimensions give the spatial size of the feature map. In specific, we expect the statistics of each feature channel to be relatively consistent both between different imagesand different locations within the same image. Therefore spatial batch normalization computes a mean and variance for each of the `C` feature channels by computing statistics over both the minibatch dimension `N` and the spatial dimensions `H` and `W`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DZ0ydd_7X7Nb"
      },
      "outputs": [],
      "source": [
        "def spatial_batchnorm_forward(x, gamma, beta, bn_param):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for spatial batch normalization.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data of shape (N, C, H, W)\n",
        "    - gamma: Scale parameter, of shape (C,)\n",
        "    - beta: Shift parameter, of shape (C,)\n",
        "    - bn_param: Dictionary with the following keys:\n",
        "      - mode: 'train' or 'test'; required\n",
        "      - eps: Constant for numeric stability\n",
        "      - momentum: Constant for running mean / variance. momentum=0 means that\n",
        "        old information is discarded completely at every time step, while\n",
        "        momentum=1 means that new information is never incorporated. The\n",
        "        default of momentum=0.9 should work well in most situations.\n",
        "      - running_mean: Array of shape (D,) giving running mean of features\n",
        "      - running_var Array of shape (D,) giving running variance of features\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output data, of shape (N, C, H, W)\n",
        "    - cache: Values needed for the backward pass\n",
        "    \"\"\"\n",
        "    out, cache = None, None\n",
        "\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the forward pass for spatial batch normalization.       #\n",
        "    #                                                                         #\n",
        "    # HINT: You can implement spatial batch normalization by calling the      #\n",
        "    # vanilla version of batch normalization you implemented above.           #\n",
        "    # Your implementation should be very short; ours is less than five lines. #\n",
        "    ###########################################################################\n",
        "    # raise NotImplementedError\n",
        "    N, C, H, W = x.shape\n",
        "    x_ = x.transpose(0, 2, 3, 1).reshape(-1, C)\n",
        "    out_, cache = batchnorm_forward(x_, gamma, beta, bn_param)\n",
        "    out = out_.reshape(N, H, W, C).transpose(0, 3, 1, 2)\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return out, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svaOyaKGX7Nb",
        "outputId": "fa897032-4615-4d56-cefc-5b048eddef5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before spatial batch normalization:\n",
            "  Shape:  (2, 3, 4, 5)\n",
            "  Means:  [9.33463814 8.90909116 9.11056338]\n",
            "  Stds:  [3.61447857 3.19347686 3.5168142 ]\n",
            "After spatial batch normalization:\n",
            "  Shape:  (2, 3, 4, 5)\n",
            "  Means:  [ 6.18949336e-16  5.99520433e-16 -1.22124533e-16]\n",
            "  Stds:  [0.99999962 0.99999951 0.9999996 ]\n",
            "After spatial batch normalization (nontrivial gamma, beta):\n",
            "  Shape:  (2, 3, 4, 5)\n",
            "  Means:  [6. 7. 8.]\n",
            "  Stds:  [2.99999885 3.99999804 4.99999798]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(231)\n",
        "# Check the training-time forward pass by checking means and variances\n",
        "# of features both before and after spatial batch normalization\n",
        "\n",
        "N, C, H, W = 2, 3, 4, 5\n",
        "x = 4 * np.random.randn(N, C, H, W) + 10\n",
        "\n",
        "print('Before spatial batch normalization:')\n",
        "print('  Shape: ', x.shape)\n",
        "print('  Means: ', x.mean(axis=(0, 2, 3)))\n",
        "print('  Stds: ', x.std(axis=(0, 2, 3)))\n",
        "\n",
        "# Means should be close to zero and stds close to one\n",
        "gamma, beta = np.ones(C), np.zeros(C)\n",
        "bn_param = {'mode': 'train'}\n",
        "out, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
        "print('After spatial batch normalization:')\n",
        "print('  Shape: ', out.shape)\n",
        "print('  Means: ', out.mean(axis=(0, 2, 3)))\n",
        "print('  Stds: ', out.std(axis=(0, 2, 3)))\n",
        "\n",
        "# Means should be close to beta and stds close to gamma\n",
        "gamma, beta = np.asarray([3, 4, 5]), np.asarray([6, 7, 8])\n",
        "out, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
        "print('After spatial batch normalization (nontrivial gamma, beta):')\n",
        "print('  Shape: ', out.shape)\n",
        "print('  Means: ', out.mean(axis=(0, 2, 3)))\n",
        "print('  Stds: ', out.std(axis=(0, 2, 3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMVSh7QPX7Nc",
        "outputId": "a2db9d6c-4940-495f-a89d-c695c1aefe43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After spatial batch normalization (test-time):\n",
            "  means:  [-0.08034406  0.07562881  0.05716371  0.04378383]\n",
            "  stds:  [0.96718744 1.0299714  1.02887624 1.00585577]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(231)\n",
        "# Check the test-time forward pass by running the training-time\n",
        "# forward pass many times to warm up the running averages, and then\n",
        "# checking the means and variances of activations after a test-time\n",
        "# forward pass.\n",
        "N, C, H, W = 10, 4, 11, 12\n",
        "\n",
        "bn_param = {'mode': 'train'}\n",
        "gamma = np.ones(C)\n",
        "beta = np.zeros(C)\n",
        "for t in range(50):\n",
        "  x = 2.3 * np.random.randn(N, C, H, W) + 13\n",
        "  spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
        "bn_param['mode'] = 'test'\n",
        "x = 2.3 * np.random.randn(N, C, H, W) + 13\n",
        "a_norm, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
        "\n",
        "# Means should be close to zero and stds close to one, but will be\n",
        "# noisier than training-time forward passes.\n",
        "print('After spatial batch normalization (test-time):')\n",
        "print('  means: ', a_norm.mean(axis=(0, 2, 3)))\n",
        "print('  stds: ', a_norm.std(axis=(0, 2, 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMs1zpyjX7Nd"
      },
      "source": [
        "## Part 2: Implementing CNNs (Convolutional Neural Networks) using PyTorch (75 points)\n",
        "<font size='4'>You may find the documentation of PyTorch useful https://pytorch.org/docs/stable/index.html."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5lX9yRwX7Nd"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.1: Implement a custom CNN (12 points).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CjD5dpkwZZkI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple convolutional network with the following architecture:\n",
        "\n",
        "    [conv - bn - relu] x M - global_average_pooling - affine - softmax\n",
        "\n",
        "    \"[conv - bn - relu] x M\" means the \"conv-bn-relu\" block is repeated for\n",
        "    M times, where M is implicitly defined by the convolution layers' parameters.\n",
        "    Whether to use the batch normalization layer (bn) in-between is a design choice.\n",
        "\n",
        "    For each convolution layer, we do downsampling of factor 2 by setting the stride\n",
        "    to be 2. So we can have a large receptive field size.\n",
        "\n",
        "    The network operates on minibatches of data that have shape (N, C, H, W)\n",
        "    consisting of N images, each with height H and width W and with C input\n",
        "    channels.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=(3, 32, 32), filter_sizes=[7], filter_channels=[32],\n",
        "            num_classes=10, use_batch_norm=True):\n",
        "        \"\"\"\n",
        "        Initialize a new CNN.\n",
        "\n",
        "        Inputs:\n",
        "        - input_dim: Tuple (C, H, W) giving size of input data\n",
        "        - filter_sizes: Width/height of filters to use in the convolutional layer. It is a\n",
        "          list whose length defines the number of convolution layers.\n",
        "        - filter_channels: Number of filters to use in each convolutional layer. It has the\n",
        "          same length as filter_sizes.\n",
        "        - num_classes: Number of output classes\n",
        "        - use_batch_norm: A boolean variable indicating whether to use batch normalization\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        assert len(filter_sizes) == len(filter_channels), \"Inconsistent filter sizes and channels.\"\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Define a set of layers according to the user input.                #\n",
        "        #                                                                          #\n",
        "        # IMPORTANT:                                                               #\n",
        "        # 1. For this assignment, you can assume that the padding of the every     #\n",
        "        # convolutional layer are chosen so that **the width and height of the     #\n",
        "        # input are preserved** (without considering the stride). You need to      #\n",
        "        # carefully set the `pad` parameter for the convolution.                   #\n",
        "        #                                                                          #\n",
        "        # 2. For each convolution layer, we use stride of 2 to do downsampling.    #\n",
        "        ############################################################################\n",
        "\n",
        "        layers = []\n",
        "        C, H, W = input_dim\n",
        "\n",
        "        for filter_size, filter_channel in zip(filter_sizes, filter_channels):\n",
        "            convolution_layer = nn.Conv2d(C, filter_channel, kernel_size=filter_size,\n",
        "                                            stride=2,\n",
        "                                            padding=filter_size//2)\n",
        "            layers.append(convolution_layer)\n",
        "\n",
        "            if use_batch_norm:\n",
        "                batch_norm_layer = nn.BatchNorm2d(filter_channel)\n",
        "                layers.append(batch_norm_layer)\n",
        "\n",
        "            relu_layer = nn.ReLU(inplace=True)\n",
        "            layers.append(relu_layer)\n",
        "\n",
        "            C = filter_channel\n",
        "\n",
        "        self.all_layers = nn.Sequential(*layers)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(C, num_classes)\n",
        "\n",
        "        # raise NotImplementedError\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = None\n",
        "        feat_before_gap = None\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Implement the forward pass for the simple convolutional net,       #\n",
        "        # computing the class scores for x and storing them in the logits          #\n",
        "        # variable. Also, store the feature map right before the global average    #\n",
        "        # pooling (GAP) layer in the feat_before_gap variable for debugging        #\n",
        "        # purpose only.                                                            #\n",
        "        ############################################################################\n",
        "\n",
        "        x = self.all_layers(x)\n",
        "\n",
        "        feat_before_gap = x\n",
        "        x = self.gap(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        logits = self.fc(x)\n",
        "\n",
        "        # raise NotImplementedError\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "        return logits, feat_before_gap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm-k2zX_cxJd",
        "outputId": "f3cf3d07-9693-4224-f36b-d268c42f42ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvNet(\n",
            "  (all_layers): Sequential(\n",
            "    (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "  )\n",
            "  (gap): AdaptiveAvgPool2d(output_size=1)\n",
            "  (fc): Linear(in_features=16, out_features=10, bias=True)\n",
            ")\n",
            "torch.Size([4, 16, 4, 4])\n"
          ]
        }
      ],
      "source": [
        "# Sanity check of the model\n",
        "model = ConvNet(filter_sizes=[3, 3, 3], filter_channels=[4, 8, 16])\n",
        "print(model)\n",
        "\n",
        "x = torch.rand((4, 3, 32, 32))\n",
        "logits, feat_before_gap = model(x)\n",
        "assert logits.shape == torch.Size([4, 10]), \"Incorrect shape for the logits\"\n",
        "print(feat_before_gap.shape)\n",
        "assert feat_before_gap.shape == torch.Size([4, 16, 4, 4]), \"Incorrect shape for the feature map before the GAP layer\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIg6XrIf20ao"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.2: Implement a function to test a CNN (6 points).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-NuN23oh6xXs"
      },
      "outputs": [],
      "source": [
        "# Function to test an already trained model\n",
        "def test_model(model, data_loader):\n",
        "    \"\"\"\n",
        "    Compute accuracy of the model.\n",
        "\n",
        "    Inputs:\n",
        "      - model: A CNN implemented in PyTorch\n",
        "      - data_loader: A data loader that will provide batched images and labels\n",
        "    \"\"\"\n",
        "\n",
        "    # set the model in evaluation mode so the batch norm layers will behave correctly\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    with torch.no_grad():\n",
        "        for batch_data in data_loader:\n",
        "            images, labels = batch_data\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            predicted = None\n",
        "            ############################################################################\n",
        "            # TODO: Compute the predicted labels of the batched input images and store #\n",
        "            # them in the predicted varaible.                                          #\n",
        "            ############################################################################\n",
        "\n",
        "            outputs, _ = model(images)\n",
        "            predicted = torch.argmax(outputs, 1)\n",
        "\n",
        "            # raise NotImplementedError\n",
        "            ############################################################################\n",
        "            #                             END OF YOUR CODE                             #\n",
        "            ############################################################################\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100 * correct // total\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R-tg8Vd26XZ"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.3: Implement a function to train and validate a CNN (11 points).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "H3lzA5IOsvtO"
      },
      "outputs": [],
      "source": [
        "def train_val_model(model, train_data_loader, val_data_loader, loss_fn, optimizer, lr_scheduler, num_epochs, print_freq=50):\n",
        "    \"\"\"\n",
        "    Training and validating a CNN model using PyTorch.\n",
        "\n",
        "    Inputs:\n",
        "      - model: A CNN implemented in PyTorch\n",
        "      - data_loader: A data loader that will provide batched images and labels\n",
        "      - loss_fn: A loss function (e.g., cross entropy loss)\n",
        "      - lr_scheduler: Learning rate scheduler\n",
        "      - num_epochs: Number of epochs in total\n",
        "      - print_freq: Frequency to print training statistics\n",
        "\n",
        "    Output:\n",
        "      - model: Trained CNN model\n",
        "    \"\"\"\n",
        "\n",
        "    for epoch_i in range(num_epochs):\n",
        "        # set the model in the train mode so the batch norm layers will behave correctly\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_total = 0.0\n",
        "        running_correct = 0.0\n",
        "        for i, batch_data in enumerate(train_data_loader):\n",
        "            # Every data instance is an image + label pair\n",
        "            images, labels = batch_data\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            predicted = None\n",
        "            ############################################################################\n",
        "            # TODO: Finish loss computation, gradient backpropagation, weight update,  #\n",
        "            # and computing the predicted labels of the input images and store them in #\n",
        "            # the predicted varaible, which will be used to monitor the training       #\n",
        "            # accuracy.                                                                #\n",
        "            #                                                                          #\n",
        "            # Note: The learning rate is updated after each **epoch**.                 #\n",
        "            ############################################################################\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs, _ = model(images)\n",
        "            loss = loss_fn(outputs, labels.to(torch.int64))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            predicted = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            # raise NotImplementedError\n",
        "            ############################################################################\n",
        "            #                             END OF YOUR CODE                             #\n",
        "            ############################################################################\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            running_total += labels.size(0)\n",
        "            running_correct += (predicted == labels).sum().item()\n",
        "            if i % print_freq == 0:    # print every certain number of mini-batches\n",
        "                running_loss = running_loss / print_freq\n",
        "                running_acc = running_correct / running_total * 100\n",
        "                last_lr = lr_scheduler.get_last_lr()[0]\n",
        "                print(f'[{epoch_i + 1}/{num_epochs}, {i + 1:5d}/{len(train_data_loader)}] loss: {running_loss:.3f} acc: {running_acc:.3f} lr: {last_lr:.5f}')\n",
        "                running_loss = 0.0\n",
        "                running_total = 0.0\n",
        "                running_correct = 0.0\n",
        "\n",
        "        # adjust the learning rate\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        val_acc = test_model(model, val_data_loader)\n",
        "        print(f'[{epoch_i + 1}/{num_epochs}] val acc: {val_acc:.3f}')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S17pDVCR3k-Z"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.4: Implement a function to set up the loss function, optimizer, and learning rate scheduler (8 points).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9Z62RTUR7GFx"
      },
      "outputs": [],
      "source": [
        "def set_up_loss_optimizer_lr_scheduler(model, learning_rate, momentum, lr_step_size, lr_gamma):\n",
        "    \"\"\"\n",
        "    In this programming assignment, we will adopt the most common choice for the optimizer:\n",
        "    SGD + momentum and learning rate scheduler: StepLR. Please refer to https://pytorch.org/docs/stable/optim.html#algorithms\n",
        "    and https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR for more details.\n",
        "    \"\"\"\n",
        "    loss_fn = None\n",
        "    optimizer = None\n",
        "    lr_scheduler = None\n",
        "\n",
        "    ############################################################################\n",
        "    # TODO: Define the loss function, optimizer (SGD + momentum), and          #\n",
        "    # learning rate scheduler (StepLR).                                        #\n",
        "    #                                                                          #\n",
        "    # Note: We expect you to set up the learning rate in an epoch-based way.   #\n",
        "    # We will run the learning rate scheduler after each epoch.                #\n",
        "    ############################################################################\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_step_size, gamma=lr_gamma)\n",
        "\n",
        "    # raise NotImplementedError\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "\n",
        "    return loss_fn, optimizer, lr_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SILUG9AvTl7e"
      },
      "outputs": [],
      "source": [
        "# no need to implement anything here\n",
        "def set_up_cifar10_data_loader(images, labels, batch_size, shuffle=True):\n",
        "    dataset = torch.utils.data.TensorDataset(torch.Tensor(images), torch.Tensor(labels))\n",
        "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWq-UEpN4irT"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.5: Train a good custom CNN (10 points).**\n",
        "\n",
        "<font size='4'>By tweaking different hyper parameters, such as number of convolution layers, number of filters (channels), learning rate, batch size, etc, you should achieve greater than 60% accuracy on the testing set **with 3 epochs using the SGD + momentum optimizer**.\n",
        "    \n",
        "<font size='4' color='red'>**Note: The total number of parameters of your custom CNN should be smaller than 180K.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3aYdXSHezba",
        "outputId": "693a2baf-40fe-4f5f-92d2-f83f53cf4c5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 99.210K\n",
            "There are 1532 batches in the training set.\n",
            "There are 32 batches in the validation set.\n",
            "There are 32 batches in the testing set.\n",
            "[1/3,     1/1532] loss: 0.047 acc: 9.375 lr: 0.00500\n",
            "[1/3,    51/1532] loss: 2.134 acc: 21.188 lr: 0.00500\n",
            "[1/3,   101/1532] loss: 1.842 acc: 31.688 lr: 0.00500\n",
            "[1/3,   151/1532] loss: 1.787 acc: 35.375 lr: 0.00500\n",
            "[1/3,   201/1532] loss: 1.726 acc: 37.062 lr: 0.00500\n",
            "[1/3,   251/1532] loss: 1.647 acc: 38.438 lr: 0.00500\n",
            "[1/3,   301/1532] loss: 1.609 acc: 40.812 lr: 0.00500\n",
            "[1/3,   351/1532] loss: 1.583 acc: 43.375 lr: 0.00500\n",
            "[1/3,   401/1532] loss: 1.559 acc: 42.250 lr: 0.00500\n",
            "[1/3,   451/1532] loss: 1.501 acc: 44.812 lr: 0.00500\n",
            "[1/3,   501/1532] loss: 1.499 acc: 45.625 lr: 0.00500\n",
            "[1/3,   551/1532] loss: 1.437 acc: 48.625 lr: 0.00500\n",
            "[1/3,   601/1532] loss: 1.485 acc: 46.250 lr: 0.00500\n",
            "[1/3,   651/1532] loss: 1.415 acc: 49.375 lr: 0.00500\n",
            "[1/3,   701/1532] loss: 1.467 acc: 45.625 lr: 0.00500\n",
            "[1/3,   751/1532] loss: 1.423 acc: 48.438 lr: 0.00500\n",
            "[1/3,   801/1532] loss: 1.392 acc: 50.750 lr: 0.00500\n",
            "[1/3,   851/1532] loss: 1.382 acc: 49.688 lr: 0.00500\n",
            "[1/3,   901/1532] loss: 1.399 acc: 48.938 lr: 0.00500\n",
            "[1/3,   951/1532] loss: 1.350 acc: 51.062 lr: 0.00500\n",
            "[1/3,  1001/1532] loss: 1.353 acc: 50.125 lr: 0.00500\n",
            "[1/3,  1051/1532] loss: 1.354 acc: 50.375 lr: 0.00500\n",
            "[1/3,  1101/1532] loss: 1.324 acc: 51.438 lr: 0.00500\n",
            "[1/3,  1151/1532] loss: 1.301 acc: 53.938 lr: 0.00500\n",
            "[1/3,  1201/1532] loss: 1.302 acc: 52.000 lr: 0.00500\n",
            "[1/3,  1251/1532] loss: 1.298 acc: 52.250 lr: 0.00500\n",
            "[1/3,  1301/1532] loss: 1.314 acc: 53.000 lr: 0.00500\n",
            "[1/3,  1351/1532] loss: 1.279 acc: 53.812 lr: 0.00500\n",
            "[1/3,  1401/1532] loss: 1.258 acc: 54.625 lr: 0.00500\n",
            "[1/3,  1451/1532] loss: 1.246 acc: 55.562 lr: 0.00500\n",
            "[1/3,  1501/1532] loss: 1.239 acc: 55.000 lr: 0.00500\n",
            "[1/3] val acc: 50.000\n",
            "[2/3,     1/1532] loss: 0.028 acc: 53.125 lr: 0.00500\n",
            "[2/3,    51/1532] loss: 1.222 acc: 55.375 lr: 0.00500\n",
            "[2/3,   101/1532] loss: 1.217 acc: 55.312 lr: 0.00500\n",
            "[2/3,   151/1532] loss: 1.136 acc: 58.500 lr: 0.00500\n",
            "[2/3,   201/1532] loss: 1.174 acc: 58.750 lr: 0.00500\n",
            "[2/3,   251/1532] loss: 1.167 acc: 57.875 lr: 0.00500\n",
            "[2/3,   301/1532] loss: 1.164 acc: 56.875 lr: 0.00500\n",
            "[2/3,   351/1532] loss: 1.197 acc: 57.375 lr: 0.00500\n",
            "[2/3,   401/1532] loss: 1.179 acc: 57.000 lr: 0.00500\n",
            "[2/3,   451/1532] loss: 1.118 acc: 59.250 lr: 0.00500\n",
            "[2/3,   501/1532] loss: 1.146 acc: 58.812 lr: 0.00500\n",
            "[2/3,   551/1532] loss: 1.164 acc: 58.000 lr: 0.00500\n",
            "[2/3,   601/1532] loss: 1.147 acc: 59.688 lr: 0.00500\n",
            "[2/3,   651/1532] loss: 1.180 acc: 57.188 lr: 0.00500\n",
            "[2/3,   701/1532] loss: 1.212 acc: 57.563 lr: 0.00500\n",
            "[2/3,   751/1532] loss: 1.146 acc: 59.062 lr: 0.00500\n",
            "[2/3,   801/1532] loss: 1.081 acc: 62.125 lr: 0.00500\n",
            "[2/3,   851/1532] loss: 1.133 acc: 59.312 lr: 0.00500\n",
            "[2/3,   901/1532] loss: 1.146 acc: 60.125 lr: 0.00500\n",
            "[2/3,   951/1532] loss: 1.087 acc: 61.687 lr: 0.00500\n",
            "[2/3,  1001/1532] loss: 1.085 acc: 62.562 lr: 0.00500\n",
            "[2/3,  1051/1532] loss: 1.107 acc: 60.562 lr: 0.00500\n",
            "[2/3,  1101/1532] loss: 1.056 acc: 62.187 lr: 0.00500\n",
            "[2/3,  1151/1532] loss: 1.115 acc: 59.625 lr: 0.00500\n",
            "[2/3,  1201/1532] loss: 1.099 acc: 59.062 lr: 0.00500\n",
            "[2/3,  1251/1532] loss: 1.069 acc: 61.375 lr: 0.00500\n",
            "[2/3,  1301/1532] loss: 1.100 acc: 60.688 lr: 0.00500\n",
            "[2/3,  1351/1532] loss: 1.132 acc: 59.938 lr: 0.00500\n",
            "[2/3,  1401/1532] loss: 1.092 acc: 60.688 lr: 0.00500\n",
            "[2/3,  1451/1532] loss: 1.056 acc: 63.062 lr: 0.00500\n",
            "[2/3,  1501/1532] loss: 1.046 acc: 62.750 lr: 0.00500\n",
            "[2/3] val acc: 61.000\n",
            "[3/3,     1/1532] loss: 0.022 acc: 65.625 lr: 0.00500\n",
            "[3/3,    51/1532] loss: 1.002 acc: 65.000 lr: 0.00500\n",
            "[3/3,   101/1532] loss: 0.973 acc: 66.625 lr: 0.00500\n",
            "[3/3,   151/1532] loss: 1.001 acc: 63.500 lr: 0.00500\n",
            "[3/3,   201/1532] loss: 1.004 acc: 64.000 lr: 0.00500\n",
            "[3/3,   251/1532] loss: 0.974 acc: 66.500 lr: 0.00500\n",
            "[3/3,   301/1532] loss: 1.007 acc: 64.188 lr: 0.00500\n",
            "[3/3,   351/1532] loss: 0.995 acc: 63.813 lr: 0.00500\n",
            "[3/3,   401/1532] loss: 1.040 acc: 61.375 lr: 0.00500\n",
            "[3/3,   451/1532] loss: 0.955 acc: 67.000 lr: 0.00500\n",
            "[3/3,   501/1532] loss: 0.985 acc: 65.500 lr: 0.00500\n",
            "[3/3,   551/1532] loss: 1.009 acc: 63.000 lr: 0.00500\n",
            "[3/3,   601/1532] loss: 1.015 acc: 62.687 lr: 0.00500\n",
            "[3/3,   651/1532] loss: 0.984 acc: 65.438 lr: 0.00500\n",
            "[3/3,   701/1532] loss: 0.954 acc: 65.688 lr: 0.00500\n",
            "[3/3,   751/1532] loss: 0.986 acc: 64.750 lr: 0.00500\n",
            "[3/3,   801/1532] loss: 1.014 acc: 63.125 lr: 0.00500\n",
            "[3/3,   851/1532] loss: 1.004 acc: 64.625 lr: 0.00500\n",
            "[3/3,   901/1532] loss: 0.959 acc: 66.312 lr: 0.00500\n",
            "[3/3,   951/1532] loss: 1.017 acc: 62.562 lr: 0.00500\n",
            "[3/3,  1001/1532] loss: 0.965 acc: 66.000 lr: 0.00500\n",
            "[3/3,  1051/1532] loss: 0.956 acc: 65.125 lr: 0.00500\n",
            "[3/3,  1101/1532] loss: 0.953 acc: 65.375 lr: 0.00500\n",
            "[3/3,  1151/1532] loss: 0.969 acc: 65.688 lr: 0.00500\n",
            "[3/3,  1201/1532] loss: 0.946 acc: 66.562 lr: 0.00500\n",
            "[3/3,  1251/1532] loss: 0.951 acc: 66.938 lr: 0.00500\n",
            "[3/3,  1301/1532] loss: 0.970 acc: 65.375 lr: 0.00500\n",
            "[3/3,  1351/1532] loss: 1.003 acc: 65.250 lr: 0.00500\n",
            "[3/3,  1401/1532] loss: 0.921 acc: 66.375 lr: 0.00500\n",
            "[3/3,  1451/1532] loss: 0.956 acc: 65.188 lr: 0.00500\n",
            "[3/3,  1501/1532] loss: 1.016 acc: 64.562 lr: 0.00500\n",
            "[3/3] val acc: 64.000\n",
            "testing accuracy: 63.000\n"
          ]
        }
      ],
      "source": [
        "# In practice, this is a hyperparameter to tune.\n",
        "# But here we use a fixed number to make the comparisons fair.\n",
        "num_epochs = 3\n",
        "\n",
        "model = None\n",
        "loss_fn = None\n",
        "optimizer = None\n",
        "lr_scheduler = None\n",
        "############################################################################\n",
        "# TODO: Set up and tune the hyper parameters.                              #\n",
        "############################################################################\n",
        "batch_size = 32\n",
        "learning_rate = 0.005\n",
        "momentum = 0.95\n",
        "lr_gamma = 0.1\n",
        "\n",
        "model = ConvNet(input_dim=(3, 32, 32), filter_sizes=[3, 3, 3, 3], filter_channels=[16, 32, 64, 128], num_classes=10)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=lr_gamma, step_size=30)\n",
        "\n",
        "############################################################################\n",
        "#                             END OF YOUR CODE                             #\n",
        "############################################################################\n",
        "\n",
        "model = model.cuda()\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print('Number of parameters: {:.3f}K'.format(num_params / 1000))\n",
        "\n",
        "# set up the data loaders\n",
        "# note the usage of the batch_size hyperparameter here\n",
        "train_loader = set_up_cifar10_data_loader(data['X_train'], data['y_train'], batch_size, shuffle=True)\n",
        "print(\"There are {} batches in the training set.\".format(len(train_loader)))\n",
        "\n",
        "val_loader = set_up_cifar10_data_loader(data['X_val'], data['y_val'], batch_size, shuffle=False)\n",
        "print(\"There are {} batches in the validation set.\".format(len(val_loader)))\n",
        "\n",
        "test_loader = set_up_cifar10_data_loader(data['X_test'], data['y_test'], batch_size, shuffle=False)\n",
        "print(\"There are {} batches in the testing set.\".format(len(test_loader)))\n",
        "\n",
        "model = train_val_model(model, train_loader, val_loader, loss_fn, optimizer, lr_scheduler, num_epochs)\n",
        "test_acc = test_model(model, test_loader)\n",
        "print(f\"testing accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvq44ocMCa8Y"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.6: Implement a ResNet-like CNN (11 points).**\n",
        "\n",
        "<font size='4'> In practice, we can borrow the existing model design for our task. ResNet (residual network) is a classical design and being used in many places. Let's experiment with it here. Since we are dealing with small images (32x32), regular ResNets are too deep with too much downsampling. We need to chop off a few blocks to reduce the depth and downsampling factor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wf64PXQ9B5Vt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from functools import partial\n",
        "from typing import Any, Callable, List, Optional, Type, Union\n",
        "from torchvision.models.resnet import conv1x1, conv3x3, BasicBlock, Bottleneck, ResNet\n",
        "\n",
        "class MyResNet(ResNet):\n",
        "    def __init__(\n",
        "        self,\n",
        "        block: Type[Union[BasicBlock, Bottleneck]],\n",
        "        layers: List[int],\n",
        "        num_classes: int = 1000,\n",
        "        zero_init_residual: bool = False,\n",
        "        groups: int = 1,\n",
        "        width_per_group: int = 64,\n",
        "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Here we will design a model architecture MyResNet, inherited from the ResNet model.\n",
        "        First check here https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py about the\n",
        "        implementation of ResNet in PyTorch.\n",
        "        What you need to do in this part is the remove the layer3 and layer4 and also modify the final\n",
        "        fully-connected layer accordingly.\n",
        "        \"\"\"\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Remove the layer3 and layer4 block in the original implementation  #\n",
        "        # of ResNet and modify the fully-connected layer (classifier) accordingly. #\n",
        "        ############################################################################\n",
        "        super().__init__(\n",
        "            block, layers, num_classes, zero_init_residual, groups,\n",
        "            width_per_group, replace_stride_with_dilation, norm_layer\n",
        "        )\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "\n",
        "        if replace_stride_with_dilation is None:\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\n",
        "                \"replace_stride_with_dilation should be None \"\n",
        "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
        "            )\n",
        "\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(128 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n",
        "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
        "                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
        "\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        logits = None\n",
        "        feat_before_gap = None\n",
        "        ############################################################################\n",
        "        # TODO: Implement the forward pass for the ResNet-like model,              #\n",
        "        # computing the class scores for x and storing them in the logits          #\n",
        "        # variable. Also, store the feature map right before the global average    #\n",
        "        # pooling (GAP) layer in the feat_before_gap variable for debugging        #\n",
        "        # purpose only.                                                            #\n",
        "        ############################################################################\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "\n",
        "        feat_before_gap = x\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        logits = self.fc(x)\n",
        "\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "        return logits, feat_before_gap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7HHhTJzi54Yz"
      },
      "outputs": [],
      "source": [
        "# Let's run a sanity check of your model\n",
        "model = MyResNet(BasicBlock, [2, 2, 2, 2], num_classes=10)\n",
        "\n",
        "x = torch.rand((4, 3, 32, 32))\n",
        "logits, feat_before_gap = model(x)\n",
        "assert logits.shape == torch.Size([4, 10]), \"Incorrect shape for the logits\"\n",
        "assert feat_before_gap.shape[2:] == torch.Size([4, 4]), \"Incorrect shape for the feature map before the GAP layer\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyYdJDdy5yAo"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.7: Train a good custom ResNet-like model (6 points).**\n",
        "\n",
        "<font size='4'>Here we use the same batch size used in the tweaking of your custom CNN. We will also simply use (part of) the ResNet18 model. You only need to tune learning rate, momentum, learning rate decay rate here. You should achieve greater than 70% accuracy on the testing set **with 3 epochs using the SGD + momentum optimizer**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kzrnj9N7DD9N",
        "outputId": "8c2a9a9e-ed35-427e-f06b-adfbb9e2752a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 11177.802K\n",
            "There are 1532 batches in the training set.\n",
            "There are 32 batches in the validation set.\n",
            "There are 32 batches in the testing set.\n",
            "[1/3,     1/1532] loss: 0.048 acc: 9.375 lr: 0.00750\n",
            "[1/3,    51/1532] loss: 2.071 acc: 24.188 lr: 0.00750\n",
            "[1/3,   101/1532] loss: 1.840 acc: 31.562 lr: 0.00750\n",
            "[1/3,   151/1532] loss: 1.704 acc: 36.438 lr: 0.00750\n",
            "[1/3,   201/1532] loss: 1.650 acc: 38.875 lr: 0.00750\n",
            "[1/3,   251/1532] loss: 1.661 acc: 39.000 lr: 0.00750\n",
            "[1/3,   301/1532] loss: 1.611 acc: 41.000 lr: 0.00750\n",
            "[1/3,   351/1532] loss: 1.547 acc: 41.938 lr: 0.00750\n",
            "[1/3,   401/1532] loss: 1.528 acc: 43.312 lr: 0.00750\n",
            "[1/3,   451/1532] loss: 1.497 acc: 46.000 lr: 0.00750\n",
            "[1/3,   501/1532] loss: 1.460 acc: 48.188 lr: 0.00750\n",
            "[1/3,   551/1532] loss: 1.458 acc: 48.875 lr: 0.00750\n",
            "[1/3,   601/1532] loss: 1.399 acc: 49.125 lr: 0.00750\n",
            "[1/3,   651/1532] loss: 1.486 acc: 46.750 lr: 0.00750\n",
            "[1/3,   701/1532] loss: 1.386 acc: 48.750 lr: 0.00750\n",
            "[1/3,   751/1532] loss: 1.349 acc: 52.000 lr: 0.00750\n",
            "[1/3,   801/1532] loss: 1.357 acc: 51.875 lr: 0.00750\n",
            "[1/3,   851/1532] loss: 1.277 acc: 55.312 lr: 0.00750\n",
            "[1/3,   901/1532] loss: 1.273 acc: 54.188 lr: 0.00750\n",
            "[1/3,   951/1532] loss: 1.302 acc: 53.938 lr: 0.00750\n",
            "[1/3,  1001/1532] loss: 1.252 acc: 56.500 lr: 0.00750\n",
            "[1/3,  1051/1532] loss: 1.247 acc: 54.562 lr: 0.00750\n",
            "[1/3,  1101/1532] loss: 1.238 acc: 55.625 lr: 0.00750\n",
            "[1/3,  1151/1532] loss: 1.196 acc: 58.375 lr: 0.00750\n",
            "[1/3,  1201/1532] loss: 1.193 acc: 56.875 lr: 0.00750\n",
            "[1/3,  1251/1532] loss: 1.142 acc: 60.000 lr: 0.00750\n",
            "[1/3,  1301/1532] loss: 1.238 acc: 55.812 lr: 0.00750\n",
            "[1/3,  1351/1532] loss: 1.236 acc: 57.000 lr: 0.00750\n",
            "[1/3,  1401/1532] loss: 1.176 acc: 57.000 lr: 0.00750\n",
            "[1/3,  1451/1532] loss: 1.215 acc: 55.625 lr: 0.00750\n",
            "[1/3,  1501/1532] loss: 1.147 acc: 58.562 lr: 0.00750\n",
            "[1/3] val acc: 42.000\n",
            "[2/3,     1/1532] loss: 0.016 acc: 78.125 lr: 0.00750\n",
            "[2/3,    51/1532] loss: 1.047 acc: 62.875 lr: 0.00750\n",
            "[2/3,   101/1532] loss: 1.079 acc: 61.750 lr: 0.00750\n",
            "[2/3,   151/1532] loss: 1.078 acc: 60.375 lr: 0.00750\n",
            "[2/3,   201/1532] loss: 1.050 acc: 62.062 lr: 0.00750\n",
            "[2/3,   251/1532] loss: 1.051 acc: 63.313 lr: 0.00750\n",
            "[2/3,   301/1532] loss: 1.067 acc: 61.062 lr: 0.00750\n",
            "[2/3,   351/1532] loss: 1.053 acc: 62.438 lr: 0.00750\n",
            "[2/3,   401/1532] loss: 1.010 acc: 63.625 lr: 0.00750\n",
            "[2/3,   451/1532] loss: 1.020 acc: 64.000 lr: 0.00750\n",
            "[2/3,   501/1532] loss: 1.039 acc: 62.687 lr: 0.00750\n",
            "[2/3,   551/1532] loss: 0.995 acc: 64.688 lr: 0.00750\n",
            "[2/3,   601/1532] loss: 0.944 acc: 66.188 lr: 0.00750\n",
            "[2/3,   651/1532] loss: 0.965 acc: 65.562 lr: 0.00750\n",
            "[2/3,   701/1532] loss: 0.970 acc: 65.562 lr: 0.00750\n",
            "[2/3,   751/1532] loss: 0.965 acc: 65.562 lr: 0.00750\n",
            "[2/3,   801/1532] loss: 1.048 acc: 63.188 lr: 0.00750\n",
            "[2/3,   851/1532] loss: 1.013 acc: 64.812 lr: 0.00750\n",
            "[2/3,   901/1532] loss: 0.978 acc: 64.625 lr: 0.00750\n",
            "[2/3,   951/1532] loss: 0.971 acc: 66.188 lr: 0.00750\n",
            "[2/3,  1001/1532] loss: 0.925 acc: 67.938 lr: 0.00750\n",
            "[2/3,  1051/1532] loss: 0.989 acc: 64.812 lr: 0.00750\n",
            "[2/3,  1101/1532] loss: 0.868 acc: 69.500 lr: 0.00750\n",
            "[2/3,  1151/1532] loss: 0.911 acc: 68.625 lr: 0.00750\n",
            "[2/3,  1201/1532] loss: 0.904 acc: 67.250 lr: 0.00750\n",
            "[2/3,  1251/1532] loss: 0.923 acc: 67.375 lr: 0.00750\n",
            "[2/3,  1301/1532] loss: 0.894 acc: 67.750 lr: 0.00750\n",
            "[2/3,  1351/1532] loss: 0.949 acc: 66.875 lr: 0.00750\n",
            "[2/3,  1401/1532] loss: 0.884 acc: 67.250 lr: 0.00750\n",
            "[2/3,  1451/1532] loss: 0.961 acc: 66.438 lr: 0.00750\n",
            "[2/3,  1501/1532] loss: 0.930 acc: 68.000 lr: 0.00750\n",
            "[2/3] val acc: 68.000\n",
            "[3/3,     1/1532] loss: 0.010 acc: 87.500 lr: 0.00750\n",
            "[3/3,    51/1532] loss: 0.836 acc: 70.188 lr: 0.00750\n",
            "[3/3,   101/1532] loss: 0.848 acc: 71.000 lr: 0.00750\n",
            "[3/3,   151/1532] loss: 0.858 acc: 69.625 lr: 0.00750\n",
            "[3/3,   201/1532] loss: 0.784 acc: 72.125 lr: 0.00750\n",
            "[3/3,   251/1532] loss: 0.861 acc: 70.562 lr: 0.00750\n",
            "[3/3,   301/1532] loss: 0.813 acc: 71.312 lr: 0.00750\n",
            "[3/3,   351/1532] loss: 0.876 acc: 68.688 lr: 0.00750\n",
            "[3/3,   401/1532] loss: 0.791 acc: 72.688 lr: 0.00750\n",
            "[3/3,   451/1532] loss: 0.869 acc: 68.688 lr: 0.00750\n",
            "[3/3,   501/1532] loss: 0.806 acc: 72.062 lr: 0.00750\n",
            "[3/3,   551/1532] loss: 0.806 acc: 71.750 lr: 0.00750\n",
            "[3/3,   601/1532] loss: 0.853 acc: 69.500 lr: 0.00750\n",
            "[3/3,   651/1532] loss: 0.794 acc: 72.688 lr: 0.00750\n",
            "[3/3,   701/1532] loss: 0.809 acc: 69.500 lr: 0.00750\n",
            "[3/3,   751/1532] loss: 0.871 acc: 69.938 lr: 0.00750\n",
            "[3/3,   801/1532] loss: 0.838 acc: 70.125 lr: 0.00750\n",
            "[3/3,   851/1532] loss: 0.761 acc: 72.312 lr: 0.00750\n",
            "[3/3,   901/1532] loss: 0.804 acc: 71.875 lr: 0.00750\n",
            "[3/3,   951/1532] loss: 0.795 acc: 71.938 lr: 0.00750\n",
            "[3/3,  1001/1532] loss: 0.826 acc: 71.062 lr: 0.00750\n",
            "[3/3,  1051/1532] loss: 0.797 acc: 72.250 lr: 0.00750\n",
            "[3/3,  1101/1532] loss: 0.781 acc: 71.875 lr: 0.00750\n",
            "[3/3,  1151/1532] loss: 0.745 acc: 74.750 lr: 0.00750\n",
            "[3/3,  1201/1532] loss: 0.710 acc: 75.625 lr: 0.00750\n",
            "[3/3,  1251/1532] loss: 0.812 acc: 72.062 lr: 0.00750\n",
            "[3/3,  1301/1532] loss: 0.809 acc: 71.938 lr: 0.00750\n",
            "[3/3,  1351/1532] loss: 0.768 acc: 73.875 lr: 0.00750\n",
            "[3/3,  1401/1532] loss: 0.760 acc: 73.625 lr: 0.00750\n",
            "[3/3,  1451/1532] loss: 0.791 acc: 73.250 lr: 0.00750\n",
            "[3/3,  1501/1532] loss: 0.732 acc: 74.250 lr: 0.00750\n",
            "[3/3] val acc: 70.000\n",
            "testing accuracy: 71.000\n"
          ]
        }
      ],
      "source": [
        "# In practice, this is a hyperparameter to tune.\n",
        "# But here we use a fixed number to make the comparisons fair.\n",
        "num_epochs = 3\n",
        "\n",
        "model = MyResNet(BasicBlock, [2, 2, 2, 2], num_classes=10)\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print('Number of parameters: {:.3f}K'.format(num_params / 1000))\n",
        "\n",
        "############################################################################\n",
        "# TODO: Set up and tune the hyper parameters.                              #\n",
        "############################################################################\n",
        "learning_rate = 0.0075\n",
        "momentum = 0.95\n",
        "lr_gamma = 0.5\n",
        "\n",
        "loss_fn = None\n",
        "optimizer = None\n",
        "lr_scheduler = None\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=lr_gamma, step_size=5)\n",
        "############################################################################\n",
        "#                             END OF YOUR CODE                             #\n",
        "############################################################################\n",
        "\n",
        "# set up the data loaders\n",
        "# note the usage of the batch_size hyperparameter here\n",
        "train_loader = set_up_cifar10_data_loader(data['X_train'], data['y_train'], batch_size, shuffle=True)\n",
        "print(\"There are {} batches in the training set.\".format(len(train_loader)))\n",
        "\n",
        "val_loader = set_up_cifar10_data_loader(data['X_val'], data['y_val'], batch_size, shuffle=False)\n",
        "print(\"There are {} batches in the validation set.\".format(len(val_loader)))\n",
        "\n",
        "test_loader = set_up_cifar10_data_loader(data['X_test'], data['y_test'], batch_size, shuffle=False)\n",
        "print(\"There are {} batches in the testing set.\".format(len(test_loader)))\n",
        "\n",
        "model = model.cuda()\n",
        "model = train_val_model(model, train_loader, val_loader, loss_fn, optimizer, lr_scheduler, num_epochs)\n",
        "test_acc = test_model(model, test_loader)\n",
        "print(f\"testing accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9yxslIX6e7l"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.8: Train a the same ResNet-like model but with ImageNet pre-trained weights (transfer learning, 8 points).**\n",
        "\n",
        "<font size='4'>Here we use the same batch size used in the tweaking of your custom CNN. We will also simply use the same ResNet18-like model. You only need to tune learning rate, momentum, learning rate decay rate here. You should achieve greater than 80% accuracy on the testing set **with 3 epochs using the SGD + momentum optimizer**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJtsipToEiJq",
        "outputId": "6680104c-f0e7-449c-9b83-91a89a064fde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 11177.802K\n",
            "There are 1532 batches in the training set.\n",
            "There are 32 batches in the validation set.\n",
            "There are 32 batches in the testing set.\n",
            "[1/3,     1/1532] loss: 0.046 acc: 9.375 lr: 0.00750\n",
            "[1/3,    51/1532] loss: 2.301 acc: 13.875 lr: 0.00750\n",
            "[1/3,   101/1532] loss: 2.262 acc: 27.187 lr: 0.00750\n",
            "[1/3,   151/1532] loss: 2.204 acc: 38.375 lr: 0.00750\n",
            "[1/3,   201/1532] loss: 2.163 acc: 42.500 lr: 0.00750\n",
            "[1/3,   251/1532] loss: 2.125 acc: 46.875 lr: 0.00750\n",
            "[1/3,   301/1532] loss: 2.089 acc: 53.938 lr: 0.00750\n",
            "[1/3,   351/1532] loss: 2.067 acc: 56.812 lr: 0.00750\n",
            "[1/3,   401/1532] loss: 2.041 acc: 58.812 lr: 0.00750\n",
            "[1/3,   451/1532] loss: 2.026 acc: 61.438 lr: 0.00750\n",
            "[1/3,   501/1532] loss: 2.025 acc: 60.312 lr: 0.00750\n",
            "[1/3,   551/1532] loss: 2.000 acc: 65.688 lr: 0.00750\n",
            "[1/3,   601/1532] loss: 1.991 acc: 66.125 lr: 0.00750\n",
            "[1/3,   651/1532] loss: 1.987 acc: 66.812 lr: 0.00750\n",
            "[1/3,   701/1532] loss: 1.983 acc: 67.000 lr: 0.00750\n",
            "[1/3,   751/1532] loss: 1.983 acc: 66.125 lr: 0.00750\n",
            "[1/3,   801/1532] loss: 1.967 acc: 67.188 lr: 0.00750\n",
            "[1/3,   851/1532] loss: 1.953 acc: 70.562 lr: 0.00750\n",
            "[1/3,   901/1532] loss: 1.961 acc: 68.812 lr: 0.00750\n",
            "[1/3,   951/1532] loss: 1.960 acc: 68.812 lr: 0.00750\n",
            "[1/3,  1001/1532] loss: 1.957 acc: 70.375 lr: 0.00750\n",
            "[1/3,  1051/1532] loss: 1.950 acc: 71.312 lr: 0.00750\n",
            "[1/3,  1101/1532] loss: 1.939 acc: 71.688 lr: 0.00750\n",
            "[1/3,  1151/1532] loss: 1.933 acc: 72.188 lr: 0.00750\n",
            "[1/3,  1201/1532] loss: 1.932 acc: 73.188 lr: 0.00750\n",
            "[1/3,  1251/1532] loss: 1.927 acc: 73.500 lr: 0.00750\n",
            "[1/3,  1301/1532] loss: 1.926 acc: 75.062 lr: 0.00750\n",
            "[1/3,  1351/1532] loss: 1.920 acc: 74.688 lr: 0.00750\n",
            "[1/3,  1401/1532] loss: 1.932 acc: 72.812 lr: 0.00750\n",
            "[1/3,  1451/1532] loss: 1.933 acc: 71.875 lr: 0.00750\n",
            "[1/3,  1501/1532] loss: 1.923 acc: 73.562 lr: 0.00750\n",
            "[1/3] val acc: 75.000\n",
            "[2/3,     1/1532] loss: 0.037 acc: 81.250 lr: 0.00750\n",
            "[2/3,    51/1532] loss: 1.901 acc: 78.188 lr: 0.00750\n",
            "[2/3,   101/1532] loss: 1.903 acc: 76.375 lr: 0.00750\n",
            "[2/3,   151/1532] loss: 1.904 acc: 77.812 lr: 0.00750\n",
            "[2/3,   201/1532] loss: 1.891 acc: 77.875 lr: 0.00750\n",
            "[2/3,   251/1532] loss: 1.898 acc: 77.688 lr: 0.00750\n",
            "[2/3,   301/1532] loss: 1.899 acc: 77.125 lr: 0.00750\n",
            "[2/3,   351/1532] loss: 1.901 acc: 76.125 lr: 0.00750\n",
            "[2/3,   401/1532] loss: 1.899 acc: 76.812 lr: 0.00750\n",
            "[2/3,   451/1532] loss: 1.898 acc: 76.812 lr: 0.00750\n",
            "[2/3,   501/1532] loss: 1.887 acc: 78.938 lr: 0.00750\n",
            "[2/3,   551/1532] loss: 1.898 acc: 76.375 lr: 0.00750\n",
            "[2/3,   601/1532] loss: 1.893 acc: 77.250 lr: 0.00750\n",
            "[2/3,   651/1532] loss: 1.888 acc: 78.125 lr: 0.00750\n",
            "[2/3,   701/1532] loss: 1.897 acc: 77.438 lr: 0.00750\n",
            "[2/3,   751/1532] loss: 1.886 acc: 79.188 lr: 0.00750\n",
            "[2/3,   801/1532] loss: 1.895 acc: 77.125 lr: 0.00750\n",
            "[2/3,   851/1532] loss: 1.884 acc: 79.875 lr: 0.00750\n",
            "[2/3,   901/1532] loss: 1.889 acc: 78.125 lr: 0.00750\n",
            "[2/3,   951/1532] loss: 1.887 acc: 78.438 lr: 0.00750\n",
            "[2/3,  1001/1532] loss: 1.891 acc: 77.438 lr: 0.00750\n",
            "[2/3,  1051/1532] loss: 1.876 acc: 79.750 lr: 0.00750\n",
            "[2/3,  1101/1532] loss: 1.885 acc: 79.625 lr: 0.00750\n",
            "[2/3,  1151/1532] loss: 1.889 acc: 77.250 lr: 0.00750\n",
            "[2/3,  1201/1532] loss: 1.889 acc: 77.500 lr: 0.00750\n",
            "[2/3,  1251/1532] loss: 1.882 acc: 79.500 lr: 0.00750\n",
            "[2/3,  1301/1532] loss: 1.863 acc: 82.750 lr: 0.00750\n",
            "[2/3,  1351/1532] loss: 1.877 acc: 80.000 lr: 0.00750\n",
            "[2/3,  1401/1532] loss: 1.884 acc: 78.250 lr: 0.00750\n",
            "[2/3,  1451/1532] loss: 1.874 acc: 79.625 lr: 0.00750\n",
            "[2/3,  1501/1532] loss: 1.874 acc: 80.438 lr: 0.00750\n",
            "[2/3] val acc: 77.000\n",
            "[3/3,     1/1532] loss: 0.037 acc: 78.125 lr: 0.00750\n",
            "[3/3,    51/1532] loss: 1.859 acc: 82.375 lr: 0.00750\n",
            "[3/3,   101/1532] loss: 1.852 acc: 83.812 lr: 0.00750\n",
            "[3/3,   151/1532] loss: 1.850 acc: 84.000 lr: 0.00750\n",
            "[3/3,   201/1532] loss: 1.856 acc: 81.875 lr: 0.00750\n",
            "[3/3,   251/1532] loss: 1.857 acc: 82.438 lr: 0.00750\n",
            "[3/3,   301/1532] loss: 1.855 acc: 83.000 lr: 0.00750\n",
            "[3/3,   351/1532] loss: 1.860 acc: 82.438 lr: 0.00750\n",
            "[3/3,   401/1532] loss: 1.872 acc: 80.250 lr: 0.00750\n",
            "[3/3,   451/1532] loss: 1.855 acc: 81.812 lr: 0.00750\n",
            "[3/3,   501/1532] loss: 1.854 acc: 82.562 lr: 0.00750\n",
            "[3/3,   551/1532] loss: 1.854 acc: 82.312 lr: 0.00750\n",
            "[3/3,   601/1532] loss: 1.858 acc: 81.125 lr: 0.00750\n",
            "[3/3,   651/1532] loss: 1.856 acc: 81.625 lr: 0.00750\n",
            "[3/3,   701/1532] loss: 1.854 acc: 81.375 lr: 0.00750\n",
            "[3/3,   751/1532] loss: 1.858 acc: 81.812 lr: 0.00750\n",
            "[3/3,   801/1532] loss: 1.848 acc: 84.125 lr: 0.00750\n",
            "[3/3,   851/1532] loss: 1.850 acc: 83.938 lr: 0.00750\n",
            "[3/3,   901/1532] loss: 1.866 acc: 80.250 lr: 0.00750\n",
            "[3/3,   951/1532] loss: 1.846 acc: 83.375 lr: 0.00750\n",
            "[3/3,  1001/1532] loss: 1.859 acc: 81.500 lr: 0.00750\n",
            "[3/3,  1051/1532] loss: 1.847 acc: 83.688 lr: 0.00750\n",
            "[3/3,  1101/1532] loss: 1.861 acc: 81.188 lr: 0.00750\n",
            "[3/3,  1151/1532] loss: 1.851 acc: 83.062 lr: 0.00750\n",
            "[3/3,  1201/1532] loss: 1.851 acc: 82.125 lr: 0.00750\n",
            "[3/3,  1251/1532] loss: 1.858 acc: 82.062 lr: 0.00750\n",
            "[3/3,  1301/1532] loss: 1.856 acc: 82.438 lr: 0.00750\n",
            "[3/3,  1351/1532] loss: 1.849 acc: 82.000 lr: 0.00750\n",
            "[3/3,  1401/1532] loss: 1.857 acc: 82.438 lr: 0.00750\n",
            "[3/3,  1451/1532] loss: 1.852 acc: 82.000 lr: 0.00750\n",
            "[3/3,  1501/1532] loss: 1.840 acc: 83.812 lr: 0.00750\n",
            "[3/3] val acc: 82.000\n",
            "testing accuracy: 81.000\n"
          ]
        }
      ],
      "source": [
        "# Let's experiment with transfer learning by borrowing the weights of a ResNet model pre-trained on ImageNet.\n",
        "import torchvision\n",
        "imagenet_resnet18 = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights)\n",
        "model = MyResNet(BasicBlock, [2, 2, 2, 2], num_classes=10)\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print('Number of parameters: {:.3f}K'.format(num_params / 1000))\n",
        "\n",
        "############################################################################\n",
        "# TODO: Copy the appropriate weights from imagenet_resnet18 to our custom  #\n",
        "# model, which shares part of the network architecture.                    #\n",
        "############################################################################\n",
        "\n",
        "model.conv1.load_state_dict(imagenet_resnet18.conv1.state_dict())\n",
        "model.bn1.load_state_dict(imagenet_resnet18.bn1.state_dict())\n",
        "model.layer1.load_state_dict(imagenet_resnet18.layer1.state_dict())\n",
        "model.layer2.load_state_dict(imagenet_resnet18.layer2.state_dict())\n",
        "\n",
        "# raise NotImplementedError\n",
        "############################################################################\n",
        "#                             END OF YOUR CODE                             #\n",
        "############################################################################\n",
        "\n",
        "############################################################################\n",
        "# TODO: Set up and tune the hyper parameters.                              #\n",
        "############################################################################\n",
        "\n",
        "learning_rate = 0.0075\n",
        "momentum = 0.95\n",
        "step_size = 5\n",
        "gamma = 0.1\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.5)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "############################################################################\n",
        "#                             END OF YOUR CODE                             #\n",
        "############################################################################\n",
        "\n",
        "# set up the data loaders\n",
        "# note the usage of the batch_size hyperparameter here\n",
        "train_loader = set_up_cifar10_data_loader(data['X_train'], data['y_train'], batch_size, shuffle=True)\n",
        "print(\"There are {} batches in the training set.\".format(len(train_loader)))\n",
        "\n",
        "val_loader = set_up_cifar10_data_loader(data['X_val'], data['y_val'], batch_size, shuffle=False)\n",
        "print(\"There are {} batches in the validation set.\".format(len(val_loader)))\n",
        "\n",
        "test_loader = set_up_cifar10_data_loader(data['X_test'], data['y_test'], batch_size, shuffle=False)\n",
        "print(\"There are {} batches in the testing set.\".format(len(test_loader)))\n",
        "\n",
        "model = model.cuda()\n",
        "model = train_val_model(model, train_loader, val_loader, loss_fn, optimizer, lr_scheduler, num_epochs=3)\n",
        "test_acc = test_model(model, test_loader)\n",
        "print(f\"testing accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KluK8QC3DiGg"
      },
      "source": [
        "<font size='4' color='red'>**Task 2.9: Briefly explain below why you got increasinly better accuracy from Task 2.5 to Task 2.8 (3 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK3NGPoxDxeS"
      },
      "source": [
        "Network Design  ResNet uses residual connections which help with smoother gradient flow and prevent issues like vanishing gradients.\n",
        "\n",
        "Feature Extraction  Compared to the custom CNN, ResNet models capture more complex patterns wchich lead to better performance.\n",
        "\n",
        "Transfer Learning  Using pre-trained weights gives the model a solid starting point so it doesn't have to learn everything from scratch leading to better accuracy in fewer epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_NOHlEorJ0J"
      },
      "source": [
        "<font size='4' color='red'> **Part 3: Extra credits (10 points).**\n",
        "\n",
        "<font size='4'> Let's do something fun here. You can do whatever you can use. Earn the full credits by achieving at least 91% accuracy on the testing set with the following restrictions:\n",
        "- Train the model for no more than 5 epochs.\n",
        "- Use a model whose number of parameters is smaller than 2M.\n",
        "- Use a convolutional neural network\n",
        "\n",
        "<font size='4' color='red'> **Note**: If you have to override any function you implemented earlier, write new code below. Do not change the function definition in previous sections so that we can grade your implementation appopriately.\n",
        "\n",
        "<font size='4' color='red'> **No partial credits will be given to this part. In other words, you won't get any credits if your final testing accuracy is lower than 91%.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVKIfuelADfK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}