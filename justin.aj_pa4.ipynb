{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "YsdSwQFUzayK",
   "metadata": {
    "id": "YsdSwQFUzayK"
   },
   "source": [
    "# Programming Assignment 4: Sequence-to-sequence Generation with Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UG-Art03zayL",
   "metadata": {
    "id": "UG-Art03zayL"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knGEE-F0zayL",
   "metadata": {
    "id": "knGEE-F0zayL"
   },
   "source": [
    "<font size='4'>In this assignment you will practice implementing a Transformer-based sequence-to-sequence (seq2seq) generation model using both Transformer encoder and decoder for machine translation (from French to English). On the one hand, you need to implement a custom Transformer decoder. On the other hand, you will also learn how to use PyTorch's built-in multi-head attention and Transformer encoder modules. In this way, you not only deepen your understanding of Transformer by implementing it on your own but also learn how to construct a model for a downstream application using PyTorch's built-in modules. After finishing this programming assignment, you will get good understandings about foundations for very state-of-the-art models that you likely see in tech news articles nowadays, like ChatGPT.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9Hjab-zayL",
   "metadata": {
    "id": "5c9Hjab-zayL"
   },
   "source": [
    "## Submission format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b502_m6FzayM",
   "metadata": {
    "id": "b502_m6FzayM"
   },
   "source": [
    "- <font size='4'>`<your_nu_username>_pa4.ipynb` with your implementations and output.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ovlu7pv1zayM",
   "metadata": {
    "id": "ovlu7pv1zayM"
   },
   "source": [
    "## Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2MCxCtR3zayM",
   "metadata": {
    "id": "2MCxCtR3zayM"
   },
   "source": [
    "<font size='4'>  \n",
    "\n",
    "- **Read the instructions and comments very carefully to avoid waste of your valuable time and deductions of points.**\n",
    "\n",
    "- You do not install any additional packages inside the Colab environment. Do not forget to choose to use GPU in the `Runtime\\Change runtime type` tab.    \n",
    "\n",
    "- **You are not allowed to look for answers online, except for the links provided in this assignment.**\n",
    "\n",
    "- **Violation of this policy will lead to failure of your course and even more severe consequences.**\n",
    "\n",
    "- Attend office hours and make posts on Piazza if you have any questions.\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34a5320",
   "metadata": {
    "id": "c34a5320"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ksCM7pd1vO-2",
   "metadata": {
    "id": "ksCM7pd1vO-2"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "gBalaRBDzayM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gBalaRBDzayM",
    "outputId": "d348ea60-1c4f-4cd1-b36e-17ccad0caca9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using the device cuda.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# We will be using the official implementation of the multi-head attention\n",
    "from torch.nn import MultiheadAttention, TransformerEncoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('We are using the device {}.'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98374904",
   "metadata": {
    "id": "98374904"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "<font size='4'> The data preparation part is largely borrowed (with modifications) from an online tutorial https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html. You are encouraged to read it and allowed to take inspirations from their implementations of the RNN-based seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4708408d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4708408d",
    "outputId": "d286deb2-dec1-4c33-94df-ad5f93d49e55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wget: /shared/centos7/anaconda3/2021.05/lib/libuuid.so.1: no version information available (required by wget)\n",
      "--2025-04-06 15:44:17--  https://download.pytorch.org/tutorial/data.zip\n",
      "Connecting to 10.99.0.130:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 2882130 (2.7M) [application/zip]\n",
      "Saving to: ‘data.zip’\n",
      "\n",
      "100%[======================================>] 2,882,130   --.-K/s   in 0.04s   \n",
      "\n",
      "2025-04-06 15:44:18 (67.2 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
      "\n",
      "Archive:  data.zip\n",
      "   creating: data/\n",
      "  inflating: data/eng-fra.txt        \n",
      "   creating: data/names/\n",
      "  inflating: data/names/Arabic.txt   \n",
      "  inflating: data/names/Chinese.txt  \n",
      "  inflating: data/names/Czech.txt    \n",
      "  inflating: data/names/Dutch.txt    \n",
      "  inflating: data/names/English.txt  \n",
      "  inflating: data/names/French.txt   \n",
      "  inflating: data/names/German.txt   \n",
      "  inflating: data/names/Greek.txt    \n",
      "  inflating: data/names/Irish.txt    \n",
      "  inflating: data/names/Italian.txt  \n",
      "  inflating: data/names/Japanese.txt  \n",
      "  inflating: data/names/Korean.txt   \n",
      "  inflating: data/names/Polish.txt   \n",
      "  inflating: data/names/Portuguese.txt  \n",
      "  inflating: data/names/Russian.txt  \n",
      "  inflating: data/names/Scottish.txt  \n",
      "  inflating: data/names/Spanish.txt  \n",
      "  inflating: data/names/Vietnamese.txt  \n"
     ]
    }
   ],
   "source": [
    "# Download the data\n",
    "!wget https://download.pytorch.org/tutorial/data.zip\n",
    "!unzip -o data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a2f06c",
   "metadata": {
    "id": "c7a2f06c"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We'll need a unique index per word to use as the inputs and targets of the networks later.\n",
    "To keep track of all this we will use a helper class called Lang which has\n",
    "word → index (word2index) and index → word (index2word) dictionaries, as well as a count\n",
    "of each word word2count which will be used to replace rare words later.\n",
    "\"\"\"\n",
    "\n",
    "# Define two special tokens here\n",
    "# SOS - start of a sentence\n",
    "# EOS - end of a sentence\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "# Note that the PAD token is not defined here, which may affect the performance, but not critically.\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        # SOS - start of sentence\n",
    "        # EOS - end of sentence\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57b57a04",
   "metadata": {
    "id": "57b57a04"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The files are all in Unicode, to simplify we will turn Unicode\n",
    "characters to ASCII, make everything lowercase, and trim most\n",
    "punctuation.\n",
    "\"\"\"\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa35885",
   "metadata": {
    "id": "eaa35885"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To read the data file we will split the file into lines, and then split lines into pairs.\n",
    "The files are all English → Other Language, so if we want to translate from\n",
    "Other Language → English I added the reverse flag to reverse the pairs.\n",
    "\"\"\"\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80c68304",
   "metadata": {
    "id": "80c68304"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Since there are a lot of example sentences and we want to train something quickly,\n",
    "we'll trim the data set to only relatively short and simple sentences.\n",
    "Here the maximum length is 10 words (that includes ending punctuation) and\n",
    "we're filtering to sentences that translate to the form \"I am\" or \"He is\" etc.\n",
    "(accounting for apostrophes replaced earlier).\n",
    "\"\"\"\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "ENG_PREFIXES = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p, max_length, eng_prefixes=None):\n",
    "    condition = len(p[0].split(' ')) < max_length and \\\n",
    "        len(p[1].split(' ')) < max_length\n",
    "    if eng_prefixes is not None:\n",
    "        condition = condition and p[1].startswith(eng_prefixes)\n",
    "    return condition\n",
    "\n",
    "def filterPairs(pairs, max_length, eng_prefixes=None):\n",
    "    return [pair for pair in pairs if filterPair(pair, max_length, eng_prefixes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03c69d2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03c69d2d",
    "outputId": "69a763b2-718d-4805-f0df-9eba42cc06d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4601\n",
      "eng 2991\n",
      "['il est si jeune', 'he s so young']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The full process for preparing the data is:\n",
    "\n",
    "- Read text file and split into lines, split lines into pairs\n",
    "- Normalize text, filter by length and content\n",
    "- Make word lists from sentences in pairs\n",
    "\"\"\"\n",
    "\n",
    "def prepareData(lang1, lang2, max_length, eng_prefixes=ENG_PREFIXES, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs, max_length, eng_prefixes)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "# Let's see what the data looks like\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', MAX_LENGTH, ENG_PREFIXES, True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e491072",
   "metadata": {
    "id": "9e491072"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To train, for each pair we will need an input tensor (indexes of the words in\n",
    "the input sentence) and target tensor (indexes of the words in the target sentence).\n",
    "While creating these vectors we will append the EOS token to both sequences.\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def convert_pairs_to_dataloader(pairs, batch_size, is_train):\n",
    "    n = len(pairs)\n",
    "    input_ids = np.ones((n, MAX_LENGTH), dtype=np.int32) * EOS_token\n",
    "    target_ids = np.ones((n, MAX_LENGTH + 1), dtype=np.int32) * EOS_token\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                         torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "    if is_train:\n",
    "        sampler = RandomSampler(data)\n",
    "        dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size, drop_last=True)#, num_workers=2)\n",
    "    else:\n",
    "        dataloader = DataLoader(data, sampler=None, batch_size=batch_size, drop_last=False)#, num_workers=2)\n",
    "    return dataloader\n",
    "\n",
    "def get_dataloader(pairs, batch_size):\n",
    "    np.random.shuffle(pairs)\n",
    "\n",
    "    n = len(pairs)\n",
    "    n_train = int(n * 0.9)\n",
    "    train_pairs = pairs[:n_train]\n",
    "    test_pairs = pairs[n_train:]\n",
    "    train_dataloader = convert_pairs_to_dataloader(train_pairs, batch_size, True)\n",
    "    test_dataloader = convert_pairs_to_dataloader(test_pairs, batch_size, False)\n",
    "\n",
    "    return train_pairs, train_dataloader, test_pairs, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d20018d",
   "metadata": {
    "id": "2d20018d"
   },
   "source": [
    "## Auxiliary Modules and Functions\n",
    "\n",
    "<font size='4'>You have implemented some of them in the third programming assignment, which are needed for this assignment, too. So their reference implementations are provided here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86b562ef",
   "metadata": {
    "id": "86b562ef"
   },
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple feedforward network. Essentially, it is a two-layer fully-connected\n",
    "    neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, ff_dim, dropout):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension\n",
    "        - ff_dim: Hidden dimension\n",
    "        \"\"\"\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "\n",
    "        ###########################################################################\n",
    "        # TODO: Define the two linear layers and a non-linear one.\n",
    "        ###########################################################################\n",
    "        self.w1 = nn.Linear(input_dim, ff_dim)\n",
    "        self.w2 = nn.Linear(ff_dim, input_dim)\n",
    "        self.non_linear = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
    "         and C is the channel dimension\n",
    "\n",
    "        Return:\n",
    "        - y: Tensor of the shape BxLxC\n",
    "        \"\"\"\n",
    "\n",
    "        y = None\n",
    "        ###########################################################################\n",
    "        # TODO: Process the input.                                                #\n",
    "        ###########################################################################\n",
    "        y = self.dropout(self.non_linear(self.w1(x)))\n",
    "        y = self.w2(y)\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3028c0af",
   "metadata": {
    "id": "3028c0af"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    A module that adds positional encoding to each of the token's features.\n",
    "    So that the Transformer is position aware.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, max_len: int=10000):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension about the features for each token\n",
    "        - max_len: The maximum sequence length\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the positional encoding and add it to x.\n",
    "\n",
    "        Input:\n",
    "        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n",
    "          and C is the channel dimension\n",
    "\n",
    "        Return:\n",
    "        - x: Tensor of the shape BxLxC, with the positional encoding added to the input\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        input_dim = x.shape[2]\n",
    "\n",
    "        pe = None\n",
    "        ###########################################################################\n",
    "        # TODO: Compute the positional encoding                                   #\n",
    "        # Check Section 3.5 for the definition (https://arxiv.org/pdf/1706.03762.pdf)\n",
    "        #                                                                         #\n",
    "        # It's a bit messy, but the definition is provided for your here for your #\n",
    "        # convenience (in LaTex).                                                 #\n",
    "        # PE_{(pos,2i)} = sin(pos / 10000^{2i/\\dmodel}) \\\\                        #\n",
    "        # PE_{(pos,2i+1)} = cos(pos / 10000^{2i/\\dmodel})                         #\n",
    "        #                                                                         #\n",
    "        # You should replace 10000 with max_len here.\n",
    "        ###########################################################################\n",
    "        norm = 10000.0 ** (torch.arange(0, self.input_dim, 2) / input_dim)\n",
    "        pos = torch.arange(seq_len).unsqueeze(1)\n",
    "        pe = torch.zeros(seq_len, input_dim)\n",
    "        pe[:, ::2]  = torch.sin(pos / norm)\n",
    "        pe[:, 1::2] = torch.cos(pos / norm)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "\n",
    "        x = x + pe.to(x.device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a21c51ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "a21c51ff",
    "outputId": "07904459-83bf-4762-eec7-c6dd6d4d0313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b630a8211c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and visualize the causal mask that is needed for training of the decoder\n",
    "# in a Transformer-based seq2seq model.\n",
    "\n",
    "def create_causal_mask(size):\n",
    "    \"Mask out subsequent positions. Also known as a causual mask.\"\n",
    "    attn_shape = (size, size)\n",
    "    causal_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(causal_mask) == 0\n",
    "\n",
    "# Let's visualize what the target mask looks like\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "\n",
    "causal_mask = create_causal_mask(20)\n",
    "print(causal_mask.shape)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(causal_mask.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DIzE2MiBzayN",
   "metadata": {
    "id": "DIzE2MiBzayN"
   },
   "source": [
    "<font size='4' color='red'> Your implementations start from here.</font>\n",
    "\n",
    "## Part 1: Implementation of the Transformer decoder and seq2seq model (60 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dlBIb4p_zayN",
   "metadata": {
    "id": "dlBIb4p_zayN"
   },
   "source": [
    "### <font size='4' color='red'>Task 1.1: Implement Transformer Decoder Cell (15 points)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "okKAcRPNzayN",
   "metadata": {
    "id": "okKAcRPNzayN"
   },
   "outputs": [],
   "source": [
    "class MyTransformerDecoderCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A single cell (unit) of the Transformer decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, dropout: float=0.1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension for each token in a sequence\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(MyTransformerDecoderCell, self).__init__()\n",
    "\n",
    "        ###########################################################################\n",
    "        # TODO: Define two multi-head attention modules using                     #\n",
    "        # nn.MultiheadAttention. See the API definition at                        #\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#\n",
    "        # Note that you need to set batch_first=True. Out of the two multi-head   #\n",
    "        # attention modules, one is for processing the tokens on the decoder side,#\n",
    "        # dong self attention. The other is doing cross attention, getting the    #\n",
    "        # attention from the decoder embeddings to the encoder embeddings. Also   #\n",
    "        # define a feedforward network. Don't forget the Dropout and LayerNorm    #\n",
    "        # layers.                                                                 #\n",
    "        #                                                                         #\n",
    "        # Note that you need to implement the post-norm variant here.             #\n",
    "        ###########################################################################\n",
    "\n",
    "        self.mha1 = nn.MultiheadAttention(input_dim, num_heads, batch_first=True)\n",
    "        self.mha2 = nn.MultiheadAttention(input_dim, num_heads, batch_first=True)\n",
    "        self.ffn = FeedForwardNetwork(input_dim, ff_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(input_dim)\n",
    "\n",
    "        # raise NotImplementedError\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "\n",
    "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, tgt_mask=None, tgt_is_causal=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: Tensor of BxLdxC, word embeddings on the decoder side\n",
    "        - encoder_output: Tensor of BxLexC, word embeddings on the encoder side\n",
    "        - tgt_mask: Tensor, causal mask for the self attention of the tokens on the\n",
    "          decoder side, which prevents each token to attend to future tokens\n",
    "        - tgt_is_causal: Boolean, indicating whether the tgt_mask is causal or not.\n",
    "          If a causal mask is provided, make sure setting tgt_is_causal=True when calling\n",
    "          the self attention module. These arguments are defined in accordance with the\n",
    "          interface of nn.TransformerDecoderCell so you can use it for debugging purpose.\n",
    "\n",
    "        Return:\n",
    "        - y: Tensor of BxLdxC. Attended features for all tokens on the decoder side.\n",
    "        \"\"\"\n",
    "\n",
    "        y = None\n",
    "        ###########################################################################\n",
    "        # TODO: Compute the self-attended features for the tokens on the decoder  #\n",
    "        # side. Then compute the cross-attended features for the tokens on the    #\n",
    "        # decoder side to the encoded features, which are finally fed into the    #\n",
    "        # feedforward network.                                                    #\n",
    "        ###########################################################################\n",
    "\n",
    "        attn1, _ = self.mha1(x, x, x, attn_mask=tgt_mask, is_causal=tgt_is_causal)\n",
    "        attn1 = self.dropout(attn1)\n",
    "        x = x + attn1\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        attn2, _ = self.mha2(x, encoder_output, encoder_output)\n",
    "        attn2 = self.dropout(attn2)\n",
    "        x = x + attn2\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        ffn_out = self.ffn(x)\n",
    "        ffn_out = self.dropout(ffn_out)\n",
    "        x = x + ffn_out\n",
    "        y = self.layer_norm(x)\n",
    "\n",
    "        # raise NotImplementedError\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "diDugP_joFSn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "diDugP_joFSn",
    "outputId": "d493fdf8-1ea8-4191-9c89-ecd22570cfc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape:  torch.Size([3, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "# Sanity check with a decoder causal mask\n",
    "dec_embed = torch.randn((3, 10, 16))\n",
    "dec_causal_mask = create_causal_mask(10) # why 10 here?\n",
    "\n",
    "enc_embed = torch.randn((3, 12, 16))\n",
    "\n",
    "model = MyTransformerDecoderCell(16, 2, 32, 0.1)\n",
    "z = model(dec_embed, enc_embed, dec_causal_mask, tgt_is_causal=True)\n",
    "assert len(z.shape) == len(dec_embed.shape)\n",
    "for dim_z, dim_x in zip(z.shape, dec_embed.shape):\n",
    "    assert dim_z == dim_x\n",
    "print('output shape: ', z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BvDmzOVPzayO",
   "metadata": {
    "id": "BvDmzOVPzayO"
   },
   "source": [
    "### <font size='4' color='red'>Task 1.2: Implement Transformer Decoder (12 points)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "KDFAYXIjzayO",
   "metadata": {
    "id": "KDFAYXIjzayO"
   },
   "outputs": [],
   "source": [
    "from typing import NoReturn\n",
    "class MyTransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A MyTransformerDecoder is a stack of multiple MyTransformerDecoderCells and a Layer Norm.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, num_cells: int, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - input_dim: Input dimension for each token in a sequence\n",
    "        - num_heads: Number of attention heads in a multi-head attention module\n",
    "        - ff_dim: The hidden dimension for a feedforward network\n",
    "        - num_cells: How many MyTransformerDecoderCells in stack\n",
    "        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n",
    "          modules.\n",
    "        \"\"\"\n",
    "        super(MyTransformerDecoder, self).__init__()\n",
    "\n",
    "        ###########################################################################\n",
    "        # TODO: Construct a nn.ModuleList to store a stack of                     #\n",
    "        # MyTranformerDecoderCells. Check the documentation here of how to use it #\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList\n",
    "        #                                                                         #\n",
    "        # At the same time, define a layer normalization layer to process the     #\n",
    "        # output of the entire decoder.                                           #\n",
    "        #                                                                         #\n",
    "        # For debugging purpose, you can replace MyTranformerDecoderCell with the #\n",
    "        # built-in nn.TranformerDecoderCell.                                      #\n",
    "        ###########################################################################\n",
    "\n",
    "        self.stack_layers = nn.ModuleList([MyTransformerDecoderCell(input_dim, num_heads, ff_dim, dropout) for _ in range(num_cells)])\n",
    "        self.layer_norm = nn.LayerNorm(input_dim)\n",
    "\n",
    "        # raise NotImplementedError\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "\n",
    "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, dec_causal_mask=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - x: Tensor of BxLdxC, word embeddings on the decoder side\n",
    "        - encoder_output: Tensor of BxLexC, word embeddings on the encoder side\n",
    "        - dec_causal_mask: Tensor, causal mask for the self attention of the tokens on the\n",
    "          decoder side, which prevents each token to attend to future tokens\n",
    "\n",
    "        Return:\n",
    "        - y: Tensor of BxLdxC. Attended features for all tokens on the decoder side.\n",
    "        \"\"\"\n",
    "\n",
    "        y = None\n",
    "        ###########################################################################\n",
    "        # TODO: Feed x into the stack of MyTransformerDecoderCells and then       #\n",
    "        # normalize the final output with layer norm.                             #\n",
    "        #                                                                         #\n",
    "        # Note: when calling TransformerDecoderCell, if a dec_causal_mask is      #\n",
    "        # provided, make sure to set tgt_is_causal=True.                          #\n",
    "        ###########################################################################\n",
    "\n",
    "\n",
    "        for layer in self.stack_layers:\n",
    "            x = layer(x, encoder_output, tgt_mask=dec_causal_mask, tgt_is_causal=dec_causal_mask is not None)\n",
    "\n",
    "        y = self.layer_norm(x)\n",
    "\n",
    "        # raise NotImplementedError\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "oJuCzlnSzayO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJuCzlnSzayO",
    "outputId": "7471de09-abbf-4cd6-9289-12c4e1d834cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape:  torch.Size([3, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "# Sanity check of tensor shapes\n",
    "dec_embed = torch.randn((3, 10, 16))\n",
    "dec_causal_mask = create_causal_mask(10)\n",
    "\n",
    "enc_embed = torch.randn((3, 12, 16))\n",
    "\n",
    "model = MyTransformerDecoder(16, 2, 32, 2, 0.1)\n",
    "z = model(dec_embed, enc_embed, dec_causal_mask)\n",
    "assert len(z.shape) == len(dec_embed.shape)\n",
    "for dim_z, dim_x in zip(z.shape, dec_embed.shape):\n",
    "    assert dim_z == dim_x\n",
    "print('output shape: ', z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718ffe76",
   "metadata": {
    "id": "718ffe76"
   },
   "source": [
    "<font size='4' color='red'>Task 1.3 (inline question): As you can see below, for a Transformer-based seq2seq model, due to the existence of the Transformer decoder, you need to implement `forward_train` and `forward_test`, corresponding its different behaviors in training and testing time. Briefly explain why you need to do so. (8 points) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27746b76",
   "metadata": {
    "id": "27746b76"
   },
   "source": [
    "[Your answer]:\n",
    "\n",
    "- During forward_train, we use the help of the ground truth and compare or compute the loss with predictions, the model has generated. This helps in efficient loss computation and parameter optimization by comparing predictions to the ground truth, which is very important for training.\n",
    "\n",
    "- During forward_test, as there is no ground truth, the decoder generates the output autoaggresively using its own predictions as input to make more predictions. This reflects real world inference where the model must produce sequences on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qx2WNnfXzayO",
   "metadata": {
    "id": "Qx2WNnfXzayO"
   },
   "source": [
    "### <font size='4' color='red'>Task 1.4: Implement a Transformer-based sequence-to-sequence model (25 points: 5 for the constructor, 10 for forward_train, and 10 for forward_test)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7frR_C_JzayO",
   "metadata": {
    "id": "7frR_C_JzayO"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based sequence-to-sequence model.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            num_encoder_layers: int, num_decoder_layers: int, embed_dim: int,\n",
    "            num_heads: int, src_vocab_size: int, tgt_vocab_size: int,\n",
    "            trx_ff_dim: int = 512, dropout: float = 0.1\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - num_encoder_layers: How many TransformerEncoderCell in stack\n",
    "        - num_decoder_layers: How many TransformerDecoderCell in stack\n",
    "        - embed_dim: Word embeddings dimension\n",
    "        - num_heads: Number of attention heads\n",
    "        - src_vocab_size: Number of tokens in the source language vocabulary\n",
    "        - tgt_vocab_size: Number of tokens in the target language vocabulary\n",
    "        - trx_ff_dim: Hidden dimension in the feedforward network\n",
    "        - dropout: Dropout ratio\n",
    "        \"\"\"\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Word embeddings for both the source and target languages\n",
    "        self.src_token_embed = nn.Embedding(src_vocab_size, embed_dim)\n",
    "        self.tgt_token_embed = nn.Embedding(tgt_vocab_size, embed_dim)\n",
    "\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.output_layer = None\n",
    "        ###########################################################################\n",
    "        # TODO: Define the positional encoding module, encoder, decoder, and the  #\n",
    "        # output layer. Think of how many classes are in the output layer.        #\n",
    "        #                                                                         #\n",
    "        # For the encoder, you'll use nn.TransformerEncoderLayer and              #\n",
    "        # nn.TransformerEncoder.  Check                                           #\n",
    "        # https://pytorch.org/docs/stable/nn.html#transformer-layers for how to   #\n",
    "        # use them.                                                               #\n",
    "        #                                                                         #\n",
    "        # For the decoder, you'll use your own MyTransformerDecoder. But for      #\n",
    "        # the debugging purpose, you can use nn.TransformerDecoderLayer and       #\n",
    "        # nn.TransformerDecoder.                                                  #\n",
    "        ###########################################################################\n",
    "\n",
    "        self.pe = PositionalEncoding(embed_dim, dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=trx_ff_dim,dropout=dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "        self.decoder = MyTransformerDecoder(input_dim=embed_dim, num_heads=num_heads, ff_dim=trx_ff_dim, num_cells=num_decoder_layers, dropout=dropout)\n",
    "        self.output_layer = nn.Linear(embed_dim, tgt_vocab_size)\n",
    "\n",
    "        # raise NotImplementedError\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "\n",
    "    def forward_train(self, src: torch.Tensor, tgt: torch.Tensor, use_causal_mask=True):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - src: Tensor of BxLe, word indexes in the source language\n",
    "        - tgt: Tensor of BxLd, word indexes in the target language\n",
    "        - use_causal_mask: Boolean, indicating whether to use a causal mask in the\n",
    "          decoder. We will use it to examine different behaviors of the model.\n",
    "\n",
    "        Return:\n",
    "        - y: Tensor of BxLdxK, corresponding to the log probabilities of generated\n",
    "             words in the target language. K is the number of classes in the output.\n",
    "        \"\"\"\n",
    "        # Get source language word embeddings. Note they are scaled.\n",
    "        src_embed = self.src_token_embed(src) * math.sqrt(self.embed_dim)\n",
    "\n",
    "        # Append an initial SOS token\n",
    "        init_token = torch.empty(src_embed.shape[0], 1, dtype=torch.long, device=tgt.device).fill_(SOS_token)\n",
    "        tgt_sentence = torch.cat((init_token, tgt), dim=1)\n",
    "        # Get target language word embeddings. Note they are scaled.\n",
    "        tgt_embed = self.tgt_token_embed(tgt_sentence) * math.sqrt(self.embed_dim)\n",
    "\n",
    "        logits = None\n",
    "        ###########################################################################\n",
    "        # TODO: Add positional encodings to the word embeddings of the source and #\n",
    "        # target languages. Then feed them to the encoder and decoder,            #\n",
    "        # respectively and get the logits finally.                                #\n",
    "        #                                                                         #\n",
    "        # Note that no for loop is allowed here.                                  #\n",
    "        ###########################################################################\n",
    "\n",
    "        tgt_embed = self.pe(tgt_embed)\n",
    "        src_embed = self.pe(src_embed)\n",
    "\n",
    "        encoder_output = self.encoder(src_embed)\n",
    "\n",
    "        if use_causal_mask:\n",
    "            device = tgt_embed.device\n",
    "            dec_causal_mask = create_causal_mask(tgt_sentence.size(1)).to(device)\n",
    "            dec_causal_mask = ~dec_causal_mask\n",
    "        else:\n",
    "            dec_causal_mask = None\n",
    "\n",
    "        decoder_output = self.decoder(tgt_embed, encoder_output, dec_causal_mask)\n",
    "        logits = self.output_layer(decoder_output)\n",
    "\n",
    "        # raise NotImplementedError\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "\n",
    "        # We discard the last token's output (the output of the EOS token), which is meaningless.\n",
    "        logits = logits[:, :-1]\n",
    "\n",
    "        # We compute the log probabilities as we will use nn.NLLLoss() as the loss function\n",
    "        return F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    def forward_test(self, src):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - src: Tensor of BxLe, word indexes in the source language\n",
    "\n",
    "        Return:\n",
    "        - y: Tensor of BxLdxK, corresponding to the log probabilities of generated\n",
    "             words in the target language. K is the number of classes in the output.\n",
    "        \"\"\"\n",
    "        # Get source language word embeddings. Note they are scaled.\n",
    "        src_embed = self.src_token_embed(src) * math.sqrt(self.embed_dim)\n",
    "\n",
    "        # For the target language generation, we always start from a SOS token.\n",
    "        decoder_input = torch.empty(src.shape[0], 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "\n",
    "        logits = None\n",
    "        ###########################################################################\n",
    "        # TODO: Add positional encodings to the word embeddings of the source     #\n",
    "        # language, from which we will generate one word at a time based on the   #\n",
    "        # previously generated ones (so called autoregressive generation) in the  #\n",
    "        # target language. We will generate MAX_LENGTH + 1 (a global variable     #\n",
    "        # defined earlier) words, **including the EOS token**, at the most using a#\n",
    "        # for loop. You may find the implementation of DecoderRNN in the tutorial #\n",
    "        # https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "        # helpful. But be aware of the differences of RNN and Transformer in      #\n",
    "        # terms of autoregressive generation.                                     #\n",
    "        ###########################################################################\n",
    "\n",
    "        src = src.to(device)\n",
    "        decoder_input = decoder_input.to(device)\n",
    "        src_embed = self.pe(src_embed)\n",
    "        encoder_output = self.encoder(src_embed)\n",
    "        batch_size = src.size(0)\n",
    "\n",
    "        logits = torch.zeros(batch_size, MAX_LENGTH + 1, self.output_layer.out_features, device=device)\n",
    "\n",
    "        for i in range(MAX_LENGTH + 1):\n",
    "            tgt_embed = self.tgt_token_embed(decoder_input) * math.sqrt(self.embed_dim)\n",
    "            tgt_embed = self.pe(tgt_embed)\n",
    "            dec_causal_mask = create_causal_mask(decoder_input.size(1)).to(device)\n",
    "            dec_causal_mask = ~dec_causal_mask\n",
    "            decoder_output = self.decoder(tgt_embed, encoder_output, dec_causal_mask)\n",
    "            current_logits = self.output_layer(decoder_output[:, -1, :])\n",
    "            logits[:, i, :] = current_logits\n",
    "            next_token = current_logits.argmax(dim=-1, keepdim=True)\n",
    "            decoder_input = torch.cat((decoder_input, next_token), dim=1)\n",
    "            if (next_token == EOS_token).all():\n",
    "                break\n",
    "\n",
    "        # raise NotImplementedError\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "\n",
    "        # Keep the returned tensor the same as the training time\n",
    "        return F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    def forward(self, src, tgt=None, use_causal_mask=True):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - src: Tensor of BxLe, word indexes in the source language\n",
    "        - tgt: Tensor of BxLd, word indexes in the target language. Can be empty.\n",
    "        - use_causal_mask: Boolean, indicating whether to use a causal mask in the\n",
    "          decoder. We will use it to examine different behaviors of the model.\n",
    "\n",
    "        Return:\n",
    "        - y: Tensor of BxLdxK, corresponding to the log probabilities of generated\n",
    "             words in the target language. K is the number of classes in the output.\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            # raining-time behavior\n",
    "            assert tgt is not None\n",
    "            return self.forward_train(src, tgt, use_causal_mask)\n",
    "\n",
    "        # testing-time behavior\n",
    "        return self.forward_test(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "pXz7nZJazayO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pXz7nZJazayO",
    "outputId": "ac38ec34-7acb-46df-d7ba-b2ceae663a63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src:  torch.Size([2, 10])\n",
      "tgt:  torch.Size([2, 9])\n",
      "output shape:  torch.Size([2, 9, 12])\n"
     ]
    }
   ],
   "source": [
    "# Sanity check of tensor shapes\n",
    "src_vocab_size = 10\n",
    "src = torch.arange(src_vocab_size).view(1, -1)\n",
    "src = torch.cat((src, src), dim=0)\n",
    "\n",
    "tgt_vocab_size = 12\n",
    "tgt = torch.arange(tgt_vocab_size - 3).view(1, -1)\n",
    "tgt = torch.cat((tgt, tgt), dim=0)\n",
    "\n",
    "print('src: ', src.shape)\n",
    "print('tgt: ', tgt.shape)\n",
    "\n",
    "model = Seq2SeqTransformer(2, 2, 16, 2, src_vocab_size, tgt_vocab_size, 32, 0.1)\n",
    "z = model(src, tgt)\n",
    "desired_output_shape = [2, 9, 12]\n",
    "assert len(z.shape) == len(desired_output_shape)\n",
    "for dim_z, dim_x in zip(z.shape, desired_output_shape):\n",
    "    assert dim_z == dim_x\n",
    "print('output shape: ', z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b258ec",
   "metadata": {
    "id": "e2b258ec"
   },
   "source": [
    "## Part 2: Train Transformer-based seq2seq models (40 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "E9lw033cpQWl",
   "metadata": {
    "id": "E9lw033cpQWl"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a helper function to print time elapsed and estimated time\n",
    "remaining given the current time and progress %.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ohpeW9TpVkN",
   "metadata": {
    "id": "1ohpeW9TpVkN"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The whole training process looks like this:\n",
    "\n",
    "- Start a timer\n",
    "- Initialize optimizers and criterion\n",
    "- Create set of training pairs\n",
    "- Start empty losses array for plotting\n",
    "Then we call train many times and occasionally print the progress (% of examples, time so far, estimated time) and average loss.\n",
    "\"\"\"\n",
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09aa4c02",
   "metadata": {
    "id": "09aa4c02"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To train we run the input sentence through the encoder, and keep track of every\n",
    "output and the latest hidden state. Then the decoder is given the <SOS> token as\n",
    " its first input, and the last hidden state of the encoder as its first hidden state.\n",
    "\n",
    "\"Teacher forcing\" is the concept of using the real target outputs as each next\n",
    "input, instead of using the decoder's guess as the next input. Using teacher\n",
    "forcing causes it to converge faster but when the trained network is exploited,\n",
    "it may exhibit instability.\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with coherent grammar\n",
    "but wander far from the correct translation -intuitively it has learned to represent\n",
    "the output grammar and can \"pick up\" the meaning once the teacher tells it the\n",
    "first few words, but it has not properly learned how to create the sentence from\n",
    "the translation in the first place.\n",
    "\n",
    "Because of the freedom PyTorch's autograd gives us, we can randomly choose to use\n",
    "teacher forcing or not with a simple if statement. Turn teacher_forcing_ratio\n",
    "up to use more of it.\n",
    "\"\"\"\n",
    "\n",
    "def train_epoch(dataloader, model, optimizer, criterion, lr_scheduler, use_causal_mask):\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        decoder_outputs = model(input_tensor, target_tensor, use_causal_mask)\n",
    "\n",
    "        # The neg-log likelihood loss of language generation is also known as perplexity.\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def train(train_dataloader, model, n_epochs, optimizer, lr_scheduler, use_causal_mask, learning_rate=0.001, print_every=100, plot_every=100):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, model, optimizer, criterion, lr_scheduler, use_causal_mask)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) loss: %.4f, lr: %.6f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg, lr_scheduler.get_last_lr()[0]))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "-mTXzFRTpioi",
   "metadata": {
    "id": "-mTXzFRTpioi"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, sentence, input_lang, output_lang):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        decoder_outputs = model(input_tensor)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words\n",
    "\n",
    "def evaluateRandomly(model, pairs, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(model, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f405eab9",
   "metadata": {
    "id": "f405eab9"
   },
   "source": [
    "<font size='4' color='red'>Task 2.1: Sanity check of fitting a small training set. (5 points) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "SQ8fwC876eUp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 819
    },
    "id": "SQ8fwC876eUp",
    "outputId": "ed0c2bb0-ac39-45ed-c0ea-77a4b78f6356"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 9207 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 3934\n",
      "eng 2542\n",
      "Number of parameters for the entire model:\t0.393M\n",
      "Number of parameters for the encoder:\t0.042M\n",
      "Number of parameters for the decoder:\t0.059M\n",
      "0m 8s (- 1m 17s) (5 10%) loss: 3.9861, lr: 0.001000\n",
      "0m 11s (- 0m 44s) (10 20%) loss: 2.0543, lr: 0.001000\n",
      "0m 13s (- 0m 32s) (15 30%) loss: 1.5932, lr: 0.001000\n",
      "0m 16s (- 0m 24s) (20 40%) loss: 1.3081, lr: 0.001000\n",
      "0m 18s (- 0m 18s) (25 50%) loss: 1.0754, lr: 0.001000\n",
      "0m 21s (- 0m 14s) (30 60%) loss: 0.8713, lr: 0.001000\n",
      "0m 24s (- 0m 10s) (35 70%) loss: 0.6963, lr: 0.001000\n",
      "0m 26s (- 0m 6s) (40 80%) loss: 0.5445, lr: 0.001000\n",
      "0m 29s (- 0m 3s) (45 90%) loss: 0.4113, lr: 0.001000\n",
      "0m 31s (- 0m 0s) (50 100%) loss: 0.3053, lr: 0.001000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "def get_num_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# re-set MAX_LENGTH\n",
    "MAX_LENGTH = 8\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', MAX_LENGTH, ENG_PREFIXES, True)\n",
    "np.random.shuffle(pairs)\n",
    "# Let's just use 1000 samples\n",
    "pairs = pairs[:1000]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EMBED_DIM = 32\n",
    "NUM_ATTN_HEADS = 4\n",
    "FF_DIM = 96\n",
    "NUM_ENCODER_LAYERS = 4\n",
    "NUM_DECODER_LAYERS = 4\n",
    "###########################################################################\n",
    "# TODO: Define the model to fit to a small training set reasonably well.  #\n",
    "# Your training loss should be around 0.5.                                #\n",
    "###########################################################################\n",
    "model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMBED_DIM,\n",
    "                           NUM_ATTN_HEADS,\n",
    "                           src_vocab_size=input_lang.n_words,\n",
    "                           tgt_vocab_size=output_lang.n_words,\n",
    "                           trx_ff_dim=FF_DIM, dropout=0.1).to(device)\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################\n",
    "\n",
    "# We will clone the model for later usage\n",
    "import copy\n",
    "model_clone = copy.deepcopy(model)\n",
    "\n",
    "print('Number of parameters for the entire model:\\t{:.3f}M'.format(get_num_params(model) / 1e6))\n",
    "print('Number of parameters for the encoder:\\t{:.3f}M'.format(get_num_params(model.encoder) / 1e6))\n",
    "print('Number of parameters for the decoder:\\t{:.3f}M'.format(get_num_params(model.decoder) / 1e6))\n",
    "\n",
    "train_pairs, train_dataloader, _, _ = get_dataloader(pairs, BATCH_SIZE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
    "\n",
    "# Your train loss should be around 0.5.\n",
    "train(train_dataloader, model, 50, optimizer, lr_scheduler, use_causal_mask=True, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "uex7i9ecptSO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uex7i9ecptSO",
    "outputId": "32ea9700-259a-4e55-ec2d-37e0be7ca48e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample output on the training set. ===\n",
      "> tu as parfaitement raison\n",
      "= you are absolutely right\n",
      "< you are absolutely right <EOS>\n",
      "\n",
      "> je suis sur que vous allez reussir\n",
      "= i m sure that you ll succeed\n",
      "< i m sure that you ll succeed <EOS>\n",
      "\n",
      "> je ne suis pas enseignant\n",
      "= i m not a teacher\n",
      "< i m not a teacher <EOS>\n",
      "\n",
      "> je prevois de jouer au football demain\n",
      "= i am going to play soccer tomorrow\n",
      "< i am going to play soccer tomorrow <EOS>\n",
      "\n",
      "> je suis ouvert aux suggestions\n",
      "= i m open to suggestions\n",
      "< i m open to suggestions <EOS>\n",
      "\n",
      "> je suis nul en langues\n",
      "= i m terrible at languages\n",
      "< i m terrible at languages <EOS>\n",
      "\n",
      "> je vais dormir\n",
      "= i am going to sleep\n",
      "< i am going to hear your worst going to\n",
      "\n",
      "> nous avons fini\n",
      "= we re finished\n",
      "< we re done <EOS>\n",
      "\n",
      "> on n a pas besoin de nous\n",
      "= we re not needed\n",
      "< we re not needed <EOS>\n",
      "\n",
      "> je ne suis pas vegetarienne\n",
      "= i m not a vegetarian\n",
      "< i m not a little funny <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's check the model's output\n",
    "print('\\n=== Sample output on the training set. ===')\n",
    "evaluateRandomly(model, train_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e85e032",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5e85e032",
    "outputId": "f836c156-0ed8-48bb-8a02-2e87a2ba13f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 2s (- 0m 26s) (5 10%) loss: 4.0005, lr: 0.001000\n",
      "0m 5s (- 0m 22s) (10 20%) loss: 2.0098, lr: 0.001000\n",
      "0m 8s (- 0m 18s) (15 30%) loss: 1.4957, lr: 0.001000\n",
      "0m 10s (- 0m 15s) (20 40%) loss: 1.1756, lr: 0.001000\n",
      "0m 13s (- 0m 13s) (25 50%) loss: 0.9231, lr: 0.001000\n",
      "0m 15s (- 0m 10s) (30 60%) loss: 0.7140, lr: 0.001000\n",
      "0m 18s (- 0m 7s) (35 70%) loss: 0.5339, lr: 0.001000\n",
      "0m 20s (- 0m 5s) (40 80%) loss: 0.3889, lr: 0.001000\n",
      "0m 23s (- 0m 2s) (45 90%) loss: 0.2758, lr: 0.001000\n",
      "0m 26s (- 0m 0s) (50 100%) loss: 0.1895, lr: 0.001000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample output on the training set. ===\n",
      "> je suis un peu desoriente\n",
      "= i m a little confused\n",
      "< you i am for <EOS>\n",
      "\n",
      "> vous etes des cretins\n",
      "= you are morons\n",
      "< you are a good a good a good a\n",
      "\n",
      "> je me prepare au pire\n",
      "= i m getting ready for the worst\n",
      "< i his than his taller his going to his\n",
      "\n",
      "> vous etes si pathetique\n",
      "= you re so pathetic\n",
      "< she s she s busy as busy as busy\n",
      "\n",
      "> vous etes de retour\n",
      "= you re back\n",
      "< you are as with this with this with this\n",
      "\n",
      "> il manque de pratique\n",
      "= he s out of practice\n",
      "< she s practice <EOS>\n",
      "\n",
      "> c est nous qui dirigeons\n",
      "= we re in charge\n",
      "< she s an she s an she s an\n",
      "\n",
      "> vous etes tres genereuses\n",
      "= you re very generous\n",
      "< she than to than to than to forward to\n",
      "\n",
      "> il est en train de pleurer\n",
      "= he s crying\n",
      "< she his going <EOS>\n",
      "\n",
      "> vous etes un sacre coco\n",
      "= you re a jolly good feller\n",
      "< she s she s she s she s she\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's re-train exactly the same model but without using the causal masks\n",
    "optimizer = optim.Adam(model_clone.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
    "\n",
    "train(train_dataloader, model_clone, 50, optimizer, lr_scheduler, use_causal_mask=False, print_every=5, plot_every=5)\n",
    "print('\\n=== Sample output on the training set. ===')\n",
    "evaluateRandomly(model_clone, train_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d36da4",
   "metadata": {
    "id": "f2d36da4"
   },
   "source": [
    "<font size='4' color='red'>Task 2.2 (inline question): Briefly summarize and explain your observation of the model's performance when it is trained without using the causal masks. (8 points) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c396a",
   "metadata": {
    "id": "f40c396a"
   },
   "source": [
    "[Your answer]:\n",
    "\n",
    "- When it is trained without using the casual masks, the generated text is nonsensical. But, with the casual masks, the generated text looks good or makes sense.\n",
    "\n",
    "- This is because, the casual masks ensures that each position in the decoder can only attend to previous positions. This helps the model to learn how to predict the next token only based on past and current information which aligns with how it will be used at test time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-no_x5TskX90",
   "metadata": {
    "id": "-no_x5TskX90"
   },
   "source": [
    "<font size='4' color='red'>Task 2.3: Train a good machine translation model. (20 points) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "A_Uq7s02khY3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "A_Uq7s02khY3",
    "outputId": "f769011a-ec18-4c18-be76-e555a47ac69e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 14636 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 6114\n",
      "eng 3701\n",
      "Number of parameters for the entire model:\t3.057M\n",
      "Number of parameters for the encoder:\t0.530M\n",
      "Number of parameters for the decoder:\t0.793M\n",
      "0m 8s (- 2m 33s) (1 5%) loss: 2.4276, lr: 0.005000\n",
      "0m 16s (- 2m 25s) (2 10%) loss: 1.7599, lr: 0.005000\n",
      "0m 24s (- 2m 16s) (3 15%) loss: 1.5204, lr: 0.005000\n",
      "0m 32s (- 2m 9s) (4 20%) loss: 1.3565, lr: 0.005000\n",
      "0m 40s (- 2m 0s) (5 25%) loss: 1.2143, lr: 0.005000\n",
      "0m 48s (- 1m 53s) (6 30%) loss: 1.0888, lr: 0.005000\n",
      "0m 56s (- 1m 45s) (7 35%) loss: 0.9627, lr: 0.005000\n",
      "1m 4s (- 1m 37s) (8 40%) loss: 0.8338, lr: 0.005000\n",
      "1m 13s (- 1m 29s) (9 45%) loss: 0.7098, lr: 0.005000\n",
      "1m 21s (- 1m 21s) (10 50%) loss: 0.5997, lr: 0.005000\n",
      "1m 29s (- 1m 13s) (11 55%) loss: 0.5163, lr: 0.005000\n",
      "1m 37s (- 1m 4s) (12 60%) loss: 0.4620, lr: 0.005000\n",
      "1m 45s (- 0m 56s) (13 65%) loss: 0.4223, lr: 0.005000\n",
      "1m 53s (- 0m 48s) (14 70%) loss: 0.3938, lr: 0.005000\n",
      "2m 1s (- 0m 40s) (15 75%) loss: 0.3716, lr: 0.005000\n",
      "2m 9s (- 0m 32s) (16 80%) loss: 0.3570, lr: 0.005000\n",
      "2m 23s (- 0m 25s) (17 85%) loss: 0.3421, lr: 0.005000\n",
      "2m 31s (- 0m 16s) (18 90%) loss: 0.3327, lr: 0.005000\n",
      "2m 39s (- 0m 8s) (19 95%) loss: 0.3212, lr: 0.005000\n",
      "2m 47s (- 0m 0s) (20 100%) loss: 0.3155, lr: 0.005000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# re-set MAX_LENGTH\n",
    "MAX_LENGTH = 8\n",
    "\n",
    "ENG_PREFIXES = (\n",
    "    \"i\"\n",
    "    \"he\",\n",
    "    \"she\",\n",
    "    \"you\",\n",
    "    \"we\",\n",
    "    \"they\"\n",
    ")\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', MAX_LENGTH, ENG_PREFIXES, True)\n",
    "np.random.shuffle(pairs)\n",
    "\n",
    "# Due to limited computing budget, we will only train the model for 20 epochs\n",
    "N_EPOCHS = 20\n",
    "\n",
    "###########################################################################\n",
    "# TODO: Tune the hyper parameters. Define the model, optimizer, and       #\n",
    "# learning rate scheduler (lr_scheduler). For your reference, the         #\n",
    "# training loss should be around 0.9. And the testing loss should be      #\n",
    "# smaller than 2.2.                                                       #\n",
    "#                                                                         #\n",
    "# Note that the **maximum** number of parameters for the entire model,    #\n",
    "# encoder, and decoder are 3.5M, 0.8M, and 1.2M, respectively.            #\n",
    "###########################################################################\n",
    "BATCH_SIZE = 32\n",
    "EMBED_DIM = 128\n",
    "NUM_ATTN_HEADS = 8\n",
    "FF_DIM = 256\n",
    "NUM_ENCODER_LAYERS = 4\n",
    "NUM_DECODER_LAYERS = 4\n",
    "LR = 0.001\n",
    "\n",
    "model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMBED_DIM,\n",
    "                           NUM_ATTN_HEADS,\n",
    "                           src_vocab_size=input_lang.n_words,\n",
    "                           tgt_vocab_size=output_lang.n_words,\n",
    "                           trx_ff_dim=FF_DIM, dropout=0.05).to(device)\n",
    "\n",
    "# **DO NOT** change this one\n",
    "train_pairs, train_dataloader, test_pairs, test_dataloader = get_dataloader(pairs, BATCH_SIZE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=5, min_lr=1e-6)\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################\n",
    "\n",
    "assert get_num_params(model) / 1e6 < 3.5\n",
    "assert get_num_params(model.encoder) / 1e6 < 0.8\n",
    "assert get_num_params(model.decoder) / 1e6 < 1.2\n",
    "print('Number of parameters for the entire model:\\t{:.3f}M'.format(get_num_params(model) / 1e6))\n",
    "print('Number of parameters for the encoder:\\t{:.3f}M'.format(get_num_params(model.encoder) / 1e6))\n",
    "print('Number of parameters for the decoder:\\t{:.3f}M'.format(get_num_params(model.decoder) / 1e6))\n",
    "\n",
    "# As a reference, your training loss should be around 0.09.\n",
    "train(train_dataloader, model, N_EPOCHS, optimizer, lr_scheduler, learning_rate=LR, use_causal_mask=True, print_every=1, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "C_hRaWHmSkh5",
   "metadata": {
    "id": "C_hRaWHmSkh5",
    "outputId": "4ca3ffc3-e252-4ef2-b30f-e1998e141ea4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:01<00:00, 26.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing loss: 2.130\n",
      "\n",
      "=== Sample output on the testing set. ===\n",
      "> elle sait tout en matiere de cuisine\n",
      "= she knows everything about cooking\n",
      "< she knows everything about cooking <EOS>\n",
      "\n",
      "> ils mentent tout le temps\n",
      "= they lie all the time\n",
      "< they lie all the time to spare <EOS>\n",
      "\n",
      "> elle est occupee\n",
      "= she s busy\n",
      "< she is busy as busy <EOS>\n",
      "\n",
      "> ils monterent des escaliers\n",
      "= they walked upstairs\n",
      "< they walked upstairs <EOS>\n",
      "\n",
      "> tom nous manque terriblement\n",
      "= we miss tom terribly\n",
      "< we miss your blood at each other <EOS>\n",
      "\n",
      "> ils s etaient entraides une fois\n",
      "= they had once helped each other\n",
      "< they were a blast for one more <EOS>\n",
      "\n",
      "> elle est tombee enceinte\n",
      "= she became pregnant\n",
      "< she has great at she s pregnant <EOS>\n",
      "\n",
      "> elle aime aussi le chocolat\n",
      "= she also likes chocolate\n",
      "< she likes too shopping around the leader <EOS>\n",
      "\n",
      "> ca se passera bien pour toi\n",
      "= you ll be fine\n",
      "< you re having a good person <EOS>\n",
      "\n",
      "> elles ont deballe leurs sandwichs\n",
      "= they unwrapped their sandwiches\n",
      "< they gave up to their meal <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def compute_inference_loss(dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader):\n",
    "            input_tensor, target_tensor = data\n",
    "\n",
    "            decoder_outputs = model(input_tensor)\n",
    "\n",
    "            loss = F.nll_loss(\n",
    "                decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "                target_tensor.view(-1)\n",
    "            )\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Your testing loss should be smaller than 2.2\n",
    "test_loss = compute_inference_loss(test_dataloader)\n",
    "print('testing loss: {:.3f}'.format(test_loss))\n",
    "\n",
    "# Let's check the model's output\n",
    "print('\\n=== Sample output on the testing set. ===')\n",
    "evaluateRandomly(model, test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "__Yvg8f0wclc",
   "metadata": {
    "id": "__Yvg8f0wclc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 411/411 [00:15<00:00, 26.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's check the inference-time loss on the training set\n",
    "train_loss = compute_inference_loss(train_dataloader)\n",
    "print('training loss: {:.3f}'.format(train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1khHw-zjwjuk",
   "metadata": {
    "id": "1khHw-zjwjuk"
   },
   "source": [
    "<font size='4' color='red'>Task 2.4 (inline question): The inference-time loss is higher than the training-time loss for the same training set. Briefly explain why. (7 points) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8MQ3zP1UwvpG",
   "metadata": {
    "id": "8MQ3zP1UwvpG"
   },
   "source": [
    "[Your answer]:\n",
    "\n",
    "This is because of the different ways the model generates predictions.\n",
    "\n",
    "During training, the model uses teacher forcing where we pass the input as the ground truth from the previous time step. The loss is then calculated by comparing the models predictions to the ground truth and as the input is always accurate the model will focus on learning the correct next token predictions.\n",
    "\n",
    "During inference, the model generates its predictions autoregressively, predicting each token based on its own previous predictions. So this increases the loss as the predictions will be different and less accurate compared to the predictions made during training as there is lack of ground truth like above, in this case making the prediction harder or the prediction may deviate further from the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8a63e-bbd2-440c-a42d-6cc5c31ea15d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
